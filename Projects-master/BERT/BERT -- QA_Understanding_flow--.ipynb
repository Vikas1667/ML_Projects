{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    " \n",
    "import torch\n",
    "# !pip install transformers\n",
    "# !pip install transformers==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "##### https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/#bert-input-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- To get pretrained Models -->\n",
    "\n",
    "### To get pretrained Models\n",
    "https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "#### tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#### model_base = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "# Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for Question Answering\n",
    "#### https://analyticsindiamag.com/10-question-answering-datasets-to-build-robust-chatbot-systems/\n",
    "#### https://lionbridge.ai/datasets/15-best-chatbot-datasets-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question='What am I losing when using extension tubes instead of a macro lens?'\n",
    "# print('sample_questions\\n',sample_question)\n",
    "sample_paragraph='''After playing around with macro photography on-the-cheap (read: reversed lens, rev. lens mounted on a straight lens, passive extension tubes), I would like to get further with this. The problems with the techniques I used is that focus is manual and aperture control is problematic at best. This limited my setup to still subjects (read: dead insects) Now, as spring is approaching, I want to be able to shoot live insects. I believe that for this, autofocus and settable aperture will be of great help.\n",
    "\n",
    "So, one obvious but expensive option is a macro lens (say, EF 100mm Macro) However, I am not really interested in yet another prime lens. An alternative is the electrical extension tubes.\n",
    "\n",
    "Except for maximum focusing distance, what am I losing when using tubes (coupled with a fine lens, say EF70-200/2.8) instead of a macro lens?\n",
    "'''\n",
    "\n",
    "# print('sample_paragraph\\n',sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "tokens_id=tokenizer.encode(sample_question,sample_paragraph)\n",
    "print(len(tokens_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Tokens to IDs\n",
    "When the BERT model was trained, each token was given a unique ID. Hence, when we want to use a pre-trained BERT model, we will first need to convert each token in the input sentence into its corresponding unique IDs.\n",
    "\n",
    "There is an important point to note when we use a pre-trained model. Since the model is pre-trained on a certain corpus, the vocabulary was also fixed. In other words, when we apply a pre-trained model to some other data, it is possible that some tokens in the new data might not appear in the fixed vocabulary of the pre-trained model. This is commonly known as the out-of-vocabulary (OOV) problem.\n",
    "\n",
    "For tokens not appearing in the original vocabulary, it is designed that they should be replaced with a special token [UNK], which stands for unknown token.\n",
    "\n",
    "However, converting all unseen tokens into [UNK] will take away a lot of information from the input data. Hence, BERT makes use of a WordPiece algorithm that breaks a word into several subwords, such that commonly seen subwords can also be represented by the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "what          2,054\n",
      "am            2,572\n",
      "i             1,045\n",
      "losing        3,974\n",
      "when          2,043\n",
      "using         2,478\n",
      "extension     5,331\n",
      "tubes        10,868\n",
      "instead       2,612\n",
      "of            1,997\n",
      "a             1,037\n",
      "macro        26,632\n",
      "lens         10,014\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "after         2,044\n",
      "playing       2,652\n",
      "around        2,105\n",
      "with          2,007\n",
      "macro        26,632\n",
      "photography   5,855\n",
      "on            2,006\n",
      "-             1,011\n",
      "the           1,996\n",
      "-             1,011\n",
      "cheap        10,036\n",
      "(             1,006\n",
      "read          3,191\n",
      ":             1,024\n",
      "reversed     11,674\n",
      "lens         10,014\n",
      ",             1,010\n",
      "rev           7,065\n",
      ".             1,012\n",
      "lens         10,014\n",
      "mounted       5,614\n",
      "on            2,006\n",
      "a             1,037\n",
      "straight      3,442\n",
      "lens         10,014\n",
      ",             1,010\n",
      "passive      13,135\n",
      "extension     5,331\n",
      "tubes        10,868\n",
      ")             1,007\n",
      ",             1,010\n",
      "i             1,045\n",
      "would         2,052\n",
      "like          2,066\n",
      "to            2,000\n",
      "get           2,131\n",
      "further       2,582\n",
      "with          2,007\n",
      "this          2,023\n",
      ".             1,012\n",
      "the           1,996\n",
      "problems      3,471\n",
      "with          2,007\n",
      "the           1,996\n",
      "techniques    5,461\n",
      "i             1,045\n",
      "used          2,109\n",
      "is            2,003\n",
      "that          2,008\n",
      "focus         3,579\n",
      "is            2,003\n",
      "manual        6,410\n",
      "and           1,998\n",
      "aperture     18,892\n",
      "control       2,491\n",
      "is            2,003\n",
      "problematic  18,636\n",
      "at            2,012\n",
      "best          2,190\n",
      ".             1,012\n",
      "this          2,023\n",
      "limited       3,132\n",
      "my            2,026\n",
      "setup        16,437\n",
      "to            2,000\n",
      "still         2,145\n",
      "subjects      5,739\n",
      "(             1,006\n",
      "read          3,191\n",
      ":             1,024\n",
      "dead          2,757\n",
      "insects       9,728\n",
      ")             1,007\n",
      "now           2,085\n",
      ",             1,010\n",
      "as            2,004\n",
      "spring        3,500\n",
      "is            2,003\n",
      "approaching   8,455\n",
      ",             1,010\n",
      "i             1,045\n",
      "want          2,215\n",
      "to            2,000\n",
      "be            2,022\n",
      "able          2,583\n",
      "to            2,000\n",
      "shoot         5,607\n",
      "live          2,444\n",
      "insects       9,728\n",
      ".             1,012\n",
      "i             1,045\n",
      "believe       2,903\n",
      "that          2,008\n",
      "for           2,005\n",
      "this          2,023\n",
      ",             1,010\n",
      "auto          8,285\n",
      "##fo         14,876\n",
      "##cus         7,874\n",
      "and           1,998\n",
      "set           2,275\n",
      "##table      10,880\n",
      "aperture     18,892\n",
      "will          2,097\n",
      "be            2,022\n",
      "of            1,997\n",
      "great         2,307\n",
      "help          2,393\n",
      ".             1,012\n",
      "so            2,061\n",
      ",             1,010\n",
      "one           2,028\n",
      "obvious       5,793\n",
      "but           2,021\n",
      "expensive     6,450\n",
      "option        5,724\n",
      "is            2,003\n",
      "a             1,037\n",
      "macro        26,632\n",
      "lens         10,014\n",
      "(             1,006\n",
      "say           2,360\n",
      ",             1,010\n",
      "e             1,041\n",
      "##f           2,546\n",
      "100           2,531\n",
      "##mm          7,382\n",
      "macro        26,632\n",
      ")             1,007\n",
      "however       2,174\n",
      ",             1,010\n",
      "i             1,045\n",
      "am            2,572\n",
      "not           2,025\n",
      "really        2,428\n",
      "interested    4,699\n",
      "in            1,999\n",
      "yet           2,664\n",
      "another       2,178\n",
      "prime         3,539\n",
      "lens         10,014\n",
      ".             1,012\n",
      "an            2,019\n",
      "alternative   4,522\n",
      "is            2,003\n",
      "the           1,996\n",
      "electrical    5,992\n",
      "extension     5,331\n",
      "tubes        10,868\n",
      ".             1,012\n",
      "except        3,272\n",
      "for           2,005\n",
      "maximum       4,555\n",
      "focusing      7,995\n",
      "distance      3,292\n",
      ",             1,010\n",
      "what          2,054\n",
      "am            2,572\n",
      "i             1,045\n",
      "losing        3,974\n",
      "when          2,043\n",
      "using         2,478\n",
      "tubes        10,868\n",
      "(             1,006\n",
      "coupled      11,211\n",
      "with          2,007\n",
      "a             1,037\n",
      "fine          2,986\n",
      "lens         10,014\n",
      ",             1,010\n",
      "say           2,360\n",
      "e             1,041\n",
      "##f           2,546\n",
      "##70         19,841\n",
      "-             1,011\n",
      "200           3,263\n",
      "/             1,013\n",
      "2             1,016\n",
      ".             1,012\n",
      "8             1,022\n",
      ")             1,007\n",
      "instead       2,612\n",
      "of            1,997\n",
      "a             1,037\n",
      "macro        26,632\n",
      "lens         10,014\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenizer.convert_ids_to_tokens(tokens_id)\n",
    "# print(tokens)\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, tokens_id):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've concatenated the question and answer_text together, but BERT still needs a way to distinguish them. BERT has two special \"Segment\" embeddings, one for segment \"A\" and one for segment \"B\". Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the question tokens, and the segment B embedding needs to be added to each of the answer_text tokens.\n",
    "\n",
    "These additions are handled for us by the transformer library, and all we need to do is specify a '0' or '1' for each token.\n",
    "\n",
    "Note: In the transformers library, huggingface likes to call these token_type_ids, but I'm going with segment_ids since this seems clearer, and is consistent with the BERT paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_index = tokens_id.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(tokens_id) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(tokens_id)\n",
    "# segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-5.2245, -4.3151, -7.1407, -7.5972, -7.2100, -8.0673, -8.5111, -7.7792,\n",
      "         -8.1569, -9.1126, -9.0503, -8.8063, -8.0988, -9.0621, -9.6507, -5.2245,\n",
      "         -6.3640, -6.6925, -7.9785, -7.9345, -3.5484, -4.8954, -7.7344, -8.7217,\n",
      "         -8.1553, -8.1698, -6.5690, -6.7664, -5.8297, -7.9816, -4.8527, -7.1700,\n",
      "         -8.5508, -5.3767, -8.7314, -6.4992, -8.0076, -8.1405, -8.0837, -6.7332,\n",
      "         -5.4148, -7.9432, -3.4901, -4.1135, -4.3120, -8.1806, -7.9794, -5.9123,\n",
      "         -7.9929, -8.0603, -8.2150, -7.8103, -7.7710, -7.9825, -6.6707, -7.5217,\n",
      "         -5.8491, -5.6869, -8.6547, -7.9563, -6.5744, -8.4726, -7.6087, -8.2721,\n",
      "         -6.2865, -0.1566, -8.1714, -4.6211, -7.8546, -0.1980, -5.1878, -8.2429,\n",
      "         -5.8505, -8.8104, -6.8585, -8.4170, -6.8890, -5.8871, -7.7531, -7.3217,\n",
      "         -8.6676, -7.0777, -6.5268, -7.8989, -6.3930, -8.4051, -5.4212, -5.9651,\n",
      "         -8.1715, -6.9076, -8.1394, -6.9000, -6.4994, -8.8077, -8.1684, -8.5116,\n",
      "         -6.1486, -6.9992, -7.5485, -7.4335, -6.4577, -7.8060, -5.9616, -6.2028,\n",
      "         -5.6882, -8.4715, -7.2162, -7.6888, -8.7054, -7.3792, -7.8897, -8.7352,\n",
      "         -4.9985, -7.0536, -7.2179, -8.4438, -5.4215, -8.6612, -5.1314, -8.5587,\n",
      "         -8.8071, -8.7355, -7.3181, -7.3936, -8.9948, -7.1544, -8.3240, -5.3398,\n",
      "         -6.8417, -7.0764, -3.9699, -6.2803, -8.7025, -7.4176, -4.7898, -5.9568,\n",
      "         -7.9697, -6.6969, -8.2836, -5.0589, -7.9835, -6.8171, -8.0417, -7.5392,\n",
      "         -8.3724, -6.8914, -7.9564, -5.0818, -7.7669, -6.6213, -7.0774, -6.5075,\n",
      "         -7.7235, -5.0771, -4.9839, -3.2106, -4.9605, -6.6181, -3.5897, -4.5307,\n",
      "         -7.3683, -4.8734, -1.9932, -2.5664, -3.5441, -4.6935,  2.0679, -2.6583,\n",
      "          7.0400,  3.1337, -0.1130, -3.8538,  0.7777, -4.8894, -5.7885, -3.0543,\n",
      "         -5.6889, -6.3054, -3.8272, -4.7576, -2.8413, -6.7120, -4.6851, -3.6310,\n",
      "         -4.6540, -8.4183, -5.1469, -2.3763, -6.2738, -6.5755, -7.2634, -5.4753,\n",
      "         -7.5950, -6.7367, -8.1160, -7.0027, -7.8627, -7.2200, -8.6707, -8.1554,\n",
      "         -6.4213, -7.7569, -8.7625, -5.2247]], grad_fn=<SqueezeBackward1>), end_logits=tensor([[-2.2681, -4.4534, -6.7947, -6.9547, -6.7462, -8.2838, -8.2548, -8.2075,\n",
      "         -6.5049, -7.7606, -7.6996, -8.1589, -7.7873, -6.6557, -7.6776, -2.2681,\n",
      "         -7.5011, -7.5201, -6.7913, -7.6629, -3.6643, -2.2352, -7.5166, -8.1108,\n",
      "         -7.8999, -7.4012, -3.8298, -7.8347, -7.9565, -7.3232, -5.4621, -4.5470,\n",
      "         -7.0126, -6.3024, -6.7624, -4.9490, -6.8317, -7.5074, -7.5252, -6.0546,\n",
      "         -2.5884, -6.5764, -4.4114, -5.0657, -1.6658, -5.0016, -5.7734, -6.6070,\n",
      "         -7.8073, -7.9147, -8.1863, -8.3614, -6.6131, -8.0314, -5.4801, -5.9971,\n",
      "         -8.2373, -7.0065, -8.0275, -7.9252, -6.0367, -7.3988, -6.3819, -7.8794,\n",
      "         -8.1529, -0.5018, -7.2174, -2.4119, -7.1844, -1.3409, -1.1862, -6.9434,\n",
      "         -2.8667, -7.5107, -3.3117, -4.6945, -7.4744, -7.2087, -7.5823, -5.9334,\n",
      "         -7.9092, -7.3986, -3.4359, -8.1602, -7.9592, -7.6660, -7.2674, -3.9003,\n",
      "         -5.3496, -6.2100, -5.3286, -8.0526, -6.5329, -7.6021, -6.5337, -6.7446,\n",
      "         -7.6916, -7.7963, -8.0901, -8.2760, -7.2724, -7.6379, -6.5138, -6.9522,\n",
      "         -2.9703, -4.6650, -7.1492, -6.8745, -7.0832, -8.1589, -6.2189, -6.6625,\n",
      "         -6.8600, -6.6275, -3.5344, -7.8920, -7.2189, -6.4735, -2.8313, -7.1927,\n",
      "         -7.4083, -8.1280, -6.9047, -5.2262, -6.2138, -7.1048, -6.4783, -7.2322,\n",
      "         -7.3802, -8.3349, -3.6693, -4.5266, -7.5913, -7.5080, -4.9591, -2.8988,\n",
      "         -7.5289, -7.8175, -6.8554, -7.1859, -7.0117, -7.0054, -5.8513, -4.9437,\n",
      "         -5.4895, -6.2154, -5.9814, -6.9096, -8.1080, -7.3845, -7.8938, -6.8125,\n",
      "         -7.8198, -7.5441, -7.0763, -4.7720, -1.8895, -3.7961, -7.3646, -4.8121,\n",
      "         -7.3469, -8.2572, -3.4167, -4.7465,  0.0404, -1.8194, -4.1961, -3.6361,\n",
      "          0.0835,  1.7442,  7.9247,  1.7252, -0.3023, -5.7039, -6.1527, -3.8380,\n",
      "         -7.9745, -7.4669, -3.1577, -7.2995, -6.2852, -7.4427, -6.9780, -5.0747,\n",
      "         -0.7903, -4.0358, -7.7590, -5.8648, -6.6704, -6.2818, -7.4182, -4.6797,\n",
      "         -6.6603, -5.0895, -6.8988, -1.8993, -2.7352, -6.2713, -6.7671, -7.2427,\n",
      "         -5.9583, -4.0546, -4.8550, -2.2684]], grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    " output= model(torch.tensor([tokens_id]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"maximum focusing distance\"\n"
     ]
    }
   ],
   "source": [
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "# tokens_id[]\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
