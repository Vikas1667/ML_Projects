{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf92005",
   "metadata": {},
   "source": [
    "### LLM\n",
    "Report aims to go into some depth on GenAI technology, the value chain, risks and uncertainties, and also considers some broad, necessary questions around the technology‚Äôs future. It is based on a combination of in-depth research, market experience, an online expert survey, and interviews with leading players from across the AI ecosystem. Below, we highlight the seven main focus areas per chapter of the Report below.\n",
    "\n",
    "[GenAI synonymous with a new civilization](https://www.adlittle.com/en/insights/report/generative-artificial-intelligence-toward-new-civilization)\n",
    "\n",
    "\n",
    "### [GenAI iimpact through Startups](https://medium.com/@sgreenman/who-will-make-money-from-the-generative-ai-gold-rush-9e8a291d2af6)\n",
    "Demand of GENAI capable systems being need in all domains of tech solutions so startups are making their way to unfold all variations to solve business problems \n",
    "\n",
    "### LLM Lifecycle\n",
    "\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/614c82ed388d53640613982e/65b7a47ae46886939395ad02_llm-project-lifecycle.webp\" width=\"600\" height=\"600\">\n",
    "\n",
    "### [Challenges with LLM](https://research.aimultiple.com/retrieval-augmented-generation/)\n",
    "\n",
    "1. Limited knowledge access and manipulation\n",
    "2. Lack of transparency\n",
    "3. Hallucinations in answers\n",
    "\n",
    "### What is future with GENAI \n",
    "[Causal Reasoning and Large Language Models: Opening a New Frontier for Causality](https://arxiv.org/abs/2305.00050)\n",
    "uesome-LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772ea3f",
   "metadata": {},
   "source": [
    "## Platform and tools to get started to understand basics \n",
    "Let unfold some components tools to get started\n",
    "\n",
    "When building a large language model (LLM) agent application, there are four key components you need: \n",
    "\n",
    "1. An agent core,\n",
    "2. A memory module,\n",
    "3. Agent tools\n",
    "4. A planning module. \n",
    "\n",
    "[AGENT](https://www.promptingguide.ai/research/llm-agents): Agent is core decision making module or system with complex reasoning capabilities, memory, and the means to execute tasks.\n",
    "\n",
    "\n",
    "### [LLM high level steps steps](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1) with implementation in [notebook](https://github.com/ray-project/llm-applications/blob/main/notebooks/rag.ipynb)\n",
    "\n",
    "    1. Pass the query to the embedding model to semantically represent it as an embedded query vector.\n",
    "    \n",
    "    2. Pass the embedded query vector to our vector DB\n",
    "    \n",
    "    3. Retrieve the top-k relevant contexts ‚Äì measured by distance between the query embedding and all the embedded chunks in our knowledge base.\n",
    "    \n",
    "    4. Pass the query text and retrieved context text to our LLM.\n",
    "    \n",
    "    5. The LLM will generate a response using the provided content.\n",
    "\n",
    "\n",
    "\n",
    "### [Different Components to know]\n",
    "\n",
    "### [RAG](https://www.promptingguide.ai/research/rag): Its retrieval component and it can be fine-tuned with its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.\n",
    "\n",
    "RAG has three key components mention [here](https://medium.com/@bijit211987/advanced-rag-for-llms-slms-5bcc6fbba411) \n",
    "    \n",
    "    1. Retriever \n",
    "    2. Reranker\n",
    "    3. Reranker\n",
    "\n",
    "\n",
    "### [Prompt for reducing Hallucination](https://medium.com/@bijit211987/advanced-prompt-engineering-for-reducing-hallucination-bb2c8ce62fc6)\n",
    "\n",
    "1. ReAct prompting\n",
    "2. Chain-of-Verification (CoVe) prompting\n",
    "3. Chain-of-Note (CoN) prompting\n",
    "4. Chain-of-Knowledge (CoK) prompting\n",
    "\n",
    "\n",
    "### How to use LLM \n",
    "#### Open source LLMs \n",
    "In [Open source LLMs overview](https://github.com/eugeneyan/open-llms) and [LLM Leaderboard](https://github.com/Hannibal046/Awesome-LLM) \n",
    "git repo consist of various open source LLM with all details and leaderboard\n",
    "\n",
    "\n",
    "#### LLMOPs \n",
    "LLMops \n",
    "\n",
    "### Links for additional \n",
    "\n",
    "[Prompt Template for OpenSource LLMs](https://medium.com/@manoranjan.rajguru/prompt-template-for-opensource-llms-9f7f6fe8ea5)\n",
    "\n",
    "[RAG Building Recipes](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b)\n",
    "\n",
    "[Advanced RAG Implementation on Custom Data Using Hybrid Search, Embed Caching And Mistral-AI](https://medium.aiplanet.com/advanced-rag-implementation-on-custom-data-using-hybrid-search-embed-caching-and-mistral-ai-ce78fdae4ef6)\n",
    "\n",
    "[Survey of RAG](https://arxiv.org/pdf/2312.10997.pdf)\n",
    "\n",
    "[Rag Techniques](https://medium.com/@bijit211987/optimizing-rag-for-llms-apps-53f6056d8118)\n",
    "\n",
    "### LLM Usecases\n",
    "[Defining Valuable GenAI Use Cases](https://medium.com/@BethanyPetryszakMudd/defining-valuable-genai-use-cases-4b8ad8495be9)\n",
    "\n",
    "### LORA \n",
    "[LORA working](https://www.run.ai/guides/generative-ai/lora-fine-tuning#:~:text=LoRA%20(Low%2DRank%20Adaptation)%20is%20a%20highly%20efficient%20method,organizations%20and%20even%20individual%20developers.)\n",
    "\n",
    "\n",
    "[huggingface lora](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "\n",
    "[Understanding LLM finetuning](https://www.run.ai/guides/generative-ai/lora-fine-tuning#:~:text=LoRA%20(Low%2DRank%20Adaptation)%20is%20a%20highly%20efficient%20method,organizations%20and%20even%20individual%20developers.)\n",
    "\n",
    "![sd](./finetuning_lora_qlora.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abeefcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9302b8e9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb27e9d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Types of GENAI\n",
    "\n",
    "\n",
    "### Application landscape\n",
    "\n",
    "\n",
    "\n",
    "### [Latent Diffusion Models](https://research.nvidia.com/labs/toronto-ai/VideoLDM/#:~:text=Latent%20Diffusion%20Models%20(LDMs)%20enable,compressed%20lower%2Ddimensional%20latent%20space.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc98ad",
   "metadata": {},
   "source": [
    "## Finetuning LLM \n",
    "\n",
    "\n",
    "### [Fine-tuning methods](https://www.superannotate.com/blog/llm-fine-tu\n",
    "ning)\n",
    "1. Instruction fine-tuning\n",
    "2. Full finetuning\n",
    "3. Parameter efficient\n",
    "4. Task specific\n",
    "5. Multi task learning\n",
    "6. Sequential fine-tuning\n",
    "7. RAG\n",
    "8. Reinforcement learning from human feedback (RLHF)\n",
    "   \n",
    "### [Finetune Openai](https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model)\n",
    "\n",
    "At a high level, fine-tuning involves the following steps:\n",
    "    \n",
    "1. prepare and upload data t\n",
    "2. Train new Finetune model d\n",
    "3. Evaluate results and if needed go to step1e\n",
    "4. Use your finetuned model \n",
    "\n",
    "\n",
    "### [Finetuning with Huggingface](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#4-fine-tune-llm-using-trl-and-the-sfttrainer)\n",
    "To train LLM with huggingface use [TLR](https://huggingface.co/docs/trl/index) and Supervised Finetuning Trainer ([SFTT](https://huggingface.co/docs/trl/main/en/sft_trainer#add-special-tokens-for-chat-format)) with  [Q-lora, Bits and bytes, 4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) and finally docker container and [Text generation inference](https://github.com/huggingface/text-generation-inference) to deploy the model  \n",
    "\n",
    "High Level steps \n",
    "\n",
    "1. Define your usecase.\n",
    "2. Setup development environment.\n",
    "3. Create and prepare the datasets.\n",
    "4. Finetune llm using trl and SFT Trainer.\n",
    "5. Test and evaluate the model\n",
    "6. Deploy the model\n",
    "\n",
    "### [Finetuning phi2](https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110)\n",
    "\n",
    "\n",
    "[data preparation](https://github.com/geronimi73/phi2-finetune/tree/main?tab=readme-ov-file)\n",
    "\n",
    "There is  lora parameters explained like rank\n",
    "\n",
    "rank: The rank in Low-Rank Adapters (LoRA) also influences the number of trainable parameters. A higher rank increases the size of the update matrices, this means more trained parameters and greater model flexibility, but at the cost of increased computational complexity. Conversely, a lower rank results in fewer parameters, leading to more efficient training and less computational burden, but potentially less flexibility in adapting the model. Thus, the choice of rank represents a trade-off between model adaptability and computational resources required for training.\n",
    "üõ† Increasing the rank from 32 to 64, for example, increases the number of trainable parameters to 9.8% (304 million) for the given target_modules.\n",
    "\n",
    "\n",
    "lora_alpha: This is a scaling factor that adjusts the influence of the low-rank updates on the original weights of the model. It modulates how much the original behaviour of the model is altered. The LoRA paper states that ‚Äútuning alpha is roughly the same as tuning the learning rate‚Äù.\n",
    "üõ† There is no consensus on how to set lora_alpha in relation to rank (reddit, Farty Pants on medium, Platypus paper, Ahead of AI blog). One approach seems to be setting lora_alpha = rank which is what we use here.\n",
    "\n",
    "\n",
    "lora_dropout: Dropout-rate during the training process. A value of 0.1 means that 10% of the trainable parameters are randomly set to non-trainable (or \"dropped\"), this should help the model generalize and prevent overfitting. 5% and 10% are common values, it does not matter which one you pick in my limited experience with this parameter.\n",
    "\n",
    "\n",
    "FAQs\n",
    "1) What is catastrophic forgetting\n",
    "2) what is Reinforcement learning from human feedback (RLHF) amd types of it\n",
    "3) what is LORA ,what are its benefits and disadvantages\n",
    "\n",
    "4)  What is [sharding](https://medium.com/@jain.sm/sharding-large-models-for-parallel-inference-ee19844cc44) for LLM\n",
    "5)  what is diffusion models\n",
    "\n",
    "\n",
    "## Finetuning LLAMA2\n",
    "https://llama.meta.com/llama-downloads/\n",
    "\n",
    "https://www.datacamp.com/tutorial/fine-tuning-llama-2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a6188",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b410f3f",
   "metadata": {},
   "source": [
    "## Diffusion models \n",
    "Diffusion models are a class of generative AI models that generate high-resolution images of varying quality. They work by gradually adding Gaussian noise to the original data in the forward diffusion process and then learning to remove the noise in the reverse diffusion process. They are latent variable models referring to a hidden continuous feature space, look similar to VAEs(Variational Autoencoders), and are loosely based on non-equilibrium thermodynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a604060",
   "metadata": {},
   "source": [
    "\n",
    "### OPtimization Techniques in LLM\n",
    "[Quantization](https://www.analyticsvidhya.com/blog/2023/11/generative-ai-with-model-quantization/) \n",
    "\n",
    "The term quantization referes to the process of mapping continuous infinite values to a smaller set of discrete finite values. In the context of LLMs, it refers to the process of converting the  weights of the model from higher precision data types to lower-precision ones.\n",
    "\n",
    "![floating](https://static.wixstatic.com/media/d9f237_e13749e4eca147d883fd97c44bf0f835~mv2.png/v1/fill/w_960,h_540,al_c,q_90,enc_auto/d9f237_e13749e4eca147d883fd97c44bf0f835~mv2.png)\n",
    "\n",
    "### Two types of Quantization techniques\n",
    "\n",
    "1. <font size=\"4\"> Post-Training Quantization(PTQ)</font>: converting the weights of an already trained model to a lower precision without any retraining. Though straightforward and easy to implement, PTQ might degrade the model's performance slightly due to the loss of precision in the value of the weights.\n",
    "2.  <font size=\"4\">Quantization Aware Training(QAT)</font>\n",
    " : Unlike PTQ, QAT integrates the weight conversion process during the training stage. This often results in superior model performance, but it's more computationally demanding. A highly used QAT technique is the QLoR\n",
    "\n",
    "### [How quantization shrinks models](https://www.tensorops.ai/post/what-are-quantized-llms)\n",
    "A typical scenario would be the reduction of the weights from FP16 (16-bit Floating-point) to INT4 (4-bit Integer). This allows for models to run on cheaper hardware and/or with higher speed. By reducing the precision of the weights, the overall quality of the LLM can also suffer some impact. \n",
    "\n",
    "Studies show that this impact varies depending on the techniques used and that larger models suffer less from change in precision. Larger models (over ~70B) are able to  maintain their capacities even when converted to 4-bit, with some techniques such as the NF4 suggesting no impact on their performance. Therefore, 4-bit appears to be the best compromise between performance and size/speed for these larger models, while 6 or 8-bit might be better for smaller models.\n",
    "\n",
    "\n",
    "A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9875b8",
   "metadata": {},
   "source": [
    "## [Langchain Stack](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "<!-- <img src=\"https://python.langchain.com/assets/images/langchain_stack-f21828069f74484521f38199910007c1.svg\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "### [Llama2](https://medium.com/@manavg/thoughts-from-llama-2-paper-b8013bab3a8)\n",
    "\n",
    "#### [Gemma](https://huggingface.co/blog/gemma) \n",
    "Gemma, a new family of state-of-the-art open LLMs, was released today by Google! It's great to see Google reinforcing its commitment to open-source AI, and we‚Äôre excited to fully support the launch with comprehensive integration in Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73ffa",
   "metadata": {},
   "source": [
    "### LLM distillation\n",
    "https://snorkel.ai/llm-distillation-techniques-to-explode-in-importance-in-2024/#:~:text=What%20is%20LLM%20distillation%3F,to%20complete%20a%20specific%20task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc42e1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://francisgichere.medium.com/cina-a-new-technique-for-causal-reasoning-in-ai-without-needing-labeled-data-e8e61f2b0e9a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdf018",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS OVERVIEW \n",
    "\n",
    "\n",
    "https://github.com/Zjh-819/LLMDataHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae686c40",
   "metadata": {},
   "source": [
    "https://www.lakera.ai/blog/large-language-models-guide\n",
    "\n",
    "1. The context gets tokenized, then vectorized or embedded\n",
    "2. The tokenized context passes through a transformer to generate the probability distribution of next words\n",
    "3. A token is selected by either:\n",
    "4. Taking the token with the largest probability, or\n",
    "5. Sampling from the output distribution\n",
    "\n",
    "https://gemini.google.com/app\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf01d7",
   "metadata": {},
   "source": [
    "\n",
    "### [multimodal](https://slds-lmu.github.io/seminar_multimodal_dl/c02-00-multimodal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f59c2b",
   "metadata": {},
   "source": [
    "# Usecase for procurement \n",
    "\n",
    "## [Transforming digital procurement through Generative AI](https://www2.deloitte.com/us/en/blog/business-operations-room-blog/2023/digital-procurement-transformation-through-generative-ai.html)\n",
    "\n",
    "### Use case 1: Contract review\n",
    "<img src=\"./transforming-digital-procurement-genai-process-flow-1.jpg\" width=\"700\" height=\"600\">\n",
    "\n",
    "\n",
    "### [causal Inference](https://francisgichere.medium.com/cina-a-new-technique-for-causal-reasoning-in-ai-without-needing-labeled-data-e8e61f2b0e9a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9d203-013f-4b8a-b521-f80cbde6934b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
