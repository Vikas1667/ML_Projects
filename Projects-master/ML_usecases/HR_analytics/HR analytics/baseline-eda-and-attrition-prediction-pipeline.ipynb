{"cells":[{"metadata":{},"cell_type":"markdown","source":"## WELCOME TO IBM HR ATTRITION EXPLORATION AND PREDICTION KERNEL\nNOTE: I'm not a native English Speaker, so sorry for any english mistakes\n\nThe main objective is to explore the data and create a model to predict the Attrition of IBM workers.\n\n### The problem\nWe will explore and try to predict the Attrition of IBM HR Analytics data. <br> \n\nWhat Is Attrition?<br>\nAttrition in business describes a gradual but deliberate reduction in staff numbers that occurs as employees retire or resign and are <b>not replaced</b>. The term is also sometimes used to describe the loss of customers or clients as they mature beyond a product or company's target market without being replaced by a younger generation.\n\nImportant: Attrition is one way a company can decrease labor costs without the disruption of layoffs.\n \n \n### Questions\nI will start with some questions that maybe will help me in exploration:\n- What's the % of Attrition at HR IBM dataset?\n- What's the distribution of Ages?\n- What's the difference between Genders?\n- The years of experience is important to Attrition ?\n- The performance or job satisfaction distributions says anything about the Attrition?\n- People that live far from the job, are more propense to Attrition?\n- And many more questions that could help us to understand the data and get some insights.\n\n### After EDA:\nI will create a Pipeline to find the model that best fit the data;\nAlso, I will create a Hyperopt model to find the best parameters to predict the Attrition of workers;\n______________________________________\n<br>\n- I hope you enjoy the Kernel. <br>\n- If you think that it is useful for you, please votes and give me your feedback =)"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats \nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Standard plotly imports\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\n\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    \n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2)\n    \n    return summary\n\ndef dummies(df, list_cols):\n    for col in list_cols:\n        df_dummies = pd.get_dummies(df[col], drop_first=True, \n                                    prefix=(str(col)))\n        df = pd.concat([df, df_dummies], axis=1)\n        df.drop(col, axis=1, inplace=True)\n        \n    return df\n\ndef get_ratio(df, cat_col):\n    attr_temp = pd.DataFrame(df.groupby([cat_col, 'Attrition'])['EmployeeNumber'].count().unstack('Attrition').reset_index())\n    attr_temp['ratio'] = round(attr_temp['Yes'] / (attr_temp['Yes'] + attr_temp['No']) * 100,2)\n    attr_temp = attr_temp[[cat_col, 'ratio']]\n    \n    return attr_temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the summary of our data"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"summary = resumetable(df_train)\n\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this first summary of data, we can see that we haven't any missing value and 3 columns have constant values.<br>\nBefore we continue, I will drop the constant features; <br>\nIt's very cool to see that we have a lot of categorical features!!! So we could have some interesting insights <br>\nThe shape of our data is (1470, 35) and the EmployeeNumber is the Id of our dataset. <br>\nAlso is important to note that entropy measure the data disorder of the each feature and it shows the information contained in this feature."},{"metadata":{},"cell_type":"markdown","source":"## Categorical features with maximum 10 unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical features\ncat_cols = ['Over18', 'StandardHours', 'EmployeeCount', 'Gender', 'PerformanceRating', \n            'OverTime', 'MaritalStatus', 'Department', 'BusinessTravel', 'StockOptionLevel', \n            'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', \n            'RelationshipSatisfaction', 'WorkLifeBalance', 'Education', \n            'JobLevel', 'EducationField', 'TrainingTimesLastYear', \n            'JobRole', 'NumCompaniesWorked']\n\n# constant features\nconst_list = ['EmployeeCount',  'Over18',  'StandardHours']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping constant features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(const_list,axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the distribution of Data Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"DATA TYPES: \")\nprint(summary['dtypes'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Nice, Now, we will start exploring the features"},{"metadata":{},"cell_type":"markdown","source":"## I will take a look on Attrition features that is our Target "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"The % distribution of Attrition features is: \")\nprint(round(df_train['Attrition'].value_counts(normalize=True),2)*100)\n\nplt.figure(figsize=(10,7))\n\ng = sns.countplot(df_train[\"Attrition\"], color='green')\ng.set_title(\"Attrition Distribution\", fontsize=22)\ng.set_ylabel('Count', fontsize=18)\ng.set_xlabel('Attrition True or False', fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool, we have 16% of true values of our target. It's a imbalanced data but nothing so terrible. <br>\nLet's keep exploring the features to see wether we can get some insights about the IBM workers"},{"metadata":{},"cell_type":"markdown","source":"## Plotting categorical features\n- First, I will plot all features that has less than 11 values; I will do it, because categories with few values are easiest to explore.\n- I will drop the constant columns and the Attrition feature.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Filtering the constant features and the target \ncat_cols = [col for col in cat_cols if col not in (const_list +['Attrition'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categoricals by ATTRITION\n-  Just features with maximum 10 values"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"UNDERSTANDING THE CATEGORICAL DISTRIBUTION BY THE TARGET (ATTRITION)\")\nprint(\"NOTE: - It's a plot just about the columns with maximum 10 values.\")\nfig, axes = plt.subplots(nrows=8, ncols=2, figsize=(18,35))\nfig.subplots_adjust(hspace=0.5, bottom=0)\n# fig.suptitle('BINARY FEATURES by the TARGET feature', fontsize=22)\n\nfor ax, catplot in zip(axes.flatten(), cat_cols):\n        sns.countplot(x=catplot, data=df_train, hue='Attrition', ax=ax, )\n        ## GEting the ratio of Years with current manager just to test into graphs\n        ax.set_title(catplot.upper(), fontsize=18)\n        ax.set_ylabel('Count', fontsize=16)\n        ax.set_xlabel(f'{catplot} Values', fontsize=15)\n        ax.legend(title='Attrition', fontsize=12)\n\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!!! We can see that some features has a high chance to Attrition. <br>\nSome features that we can see that the Attrition has different patterns is: stockoptionlevel,  performancerating, overtime, department, jobinvolvement"},{"metadata":{},"cell_type":"markdown","source":"## Continuous and large categorical features\n- Now I will explore the features with more than 10 values and try get some insights using the Attrition to see the different patterns"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# PercentSalaryHike, DistanceFromHome, Age\nprint(f'Minimum age on dataset is {df_train[\"Age\"].min()} and the maximum age is {df_train[\"Age\"].max()}')\nplt.figure(figsize=(16,22))\n\nplt.subplot(3,1,1)\ng = sns.distplot(df_train[df_train['Attrition'] == 'Yes']['Age'], label='Yes')\ng = sns.distplot(df_train[df_train['Attrition'] == 'No']['Age'], label=\"No\")\ng.set_xticklabels(g.get_xticklabels(),rotation=0)\ng.legend(title='Attrition')\ng.set_title(\"Age Distribution by Attrition\", fontsize=22)\ng.set_xlabel(\"Age Distribution\", fontsize=18)\ng.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(3,1,2)\ng1 = sns.violinplot(x='DistanceFromHome', y='Age', hue='Attrition', \n                    split=True, data=df_train, size=3)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=0)\ng1.set_title(\"Distance From Home Distribution by Attrition and Age\", fontsize=22)\ng1.set_xlabel(\"Distance From Home\", fontsize=18)\ng1.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.subplot(3,1,3)\ng2 = sns.violinplot(x='PercentSalaryHike', y='Age',\n                    split=True, hue='Attrition',data=df_train)\ng2.set_xticklabels(g2.get_xticklabels(),rotation=0)\ng2.set_title(\"Percent Salary Hike Distribution by Attrition and Age\", fontsize=22)\ng2.set_xlabel(\"Percent Salary Hike\", fontsize=18)\ng2.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.subplots_adjust(hspace = 0.4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very insightful informations!! <br>\nBased on charts we can see that the attrition is more probable in yougest people;"},{"metadata":{"trusted":true},"cell_type":"code","source":"medimum_feats = ['PercentSalaryHike', 'YearsSinceLastPromotion', 'YearsWithCurrManager', \n                 'YearsInCurrentRole', 'DistanceFromHome']\n                 \nbig_feats = [ 'YearsAtCompany', 'TotalWorkingYears', 'Age','HourlyRate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Job Features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,22))\n\nplt.subplot(3,1,1)\ng = sns.violinplot(x='YearsInCurrentRole', y= 'Age', \n                   split=True, hue='Attrition', data=df_train)\ng.set_xticklabels(g.get_xticklabels(),rotation=0)\ng.set_title(\"Years In Current Role by Attrition and Age\", fontsize=22)\ng.set_xlabel(\"Years In Current Role\", fontsize=18)\ng.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.subplot(3,1,2)\ng1 = sns.violinplot(x='YearsSinceLastPromotion', y= 'Age', \n                    split=True, hue='Attrition', data=df_train)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=0)\ng1.set_title(\"Years Since Last Promotion Distribution by Attrition and Age\", fontsize=22)\ng1.set_xlabel(\"Years since last Promotion\", fontsize=18)\ng1.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.subplot(3,1,3)\ng2 = sns.violinplot(x='YearsAtCompany', y= 'Age', \n                   split=True, hue='Attrition', data=df_train)\ng2.set_xticklabels(g2.get_xticklabels(),rotation=0)\ng2.set_title(\"Years At Company by Attrition and Age\", fontsize=22)\ng2.set_xlabel(\"Years In Current Role\", fontsize=18)\ng2.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.subplots_adjust(hspace = 0.4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. On Years at Company we can see that people with more than 12 years are less propense to leave the company.<br>\nThe same pattern we can see on Years in current Role... After 15 years in the current role, we don't have siginificant number of Attrition; \n"},{"metadata":{},"cell_type":"markdown","source":"## I will try some visuals with the Monthly Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The minimum value Income in dataset is {df_train['MonthlyIncome'].min()} and maximum {df_train['MonthlyIncome'].max()}\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Monthly Income Quantiles Distribution: \")\nprint(df_train['MonthlyIncome'].quantile([.01, .25, .5, .75, .99]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_ratio(df, cat_col):\n    attr_temp = df.groupby([cat_col, 'Attrition'])['EmployeeNumber'].count().unstack('Attrition').reset_index()\n    attr_temp['ratio'] = round(attr_temp['Yes'] / (attr_temp['Yes'] + attr_temp['No']) * 100,2)\n    attr_temp = attr_temp[[cat_col, 'ratio']]\n    \n    return attr_temp","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,22))\n\nplt.subplot(3,1,1)\n\ng = sns.violinplot(x='YearsWithCurrManager', y= 'MonthlyIncome', \n                    hue='Attrition',data=df_train, split=True)\ng.set_xticklabels(g.get_xticklabels(),rotation=0)\ng.set_title(\"Years With Current Manager Distribution by Attrition and Monthly Income\", fontsize=22)\ng.set_xlabel(\"Years with current Manager\", fontsize=18)\ng.set_ylabel(\"Monthly Income Distribution\", fontsize=18)\nax2 = g.twinx()\nattr_temp = get_ratio(df_train, 'YearsWithCurrManager')\ngg = sns.lineplot(x='YearsWithCurrManager', y= 'ratio', ax=ax2, lw=3, markers='o',\n             label=\"Attrition %\", color='black',\n             data=attr_temp)\ngg.legend( loc = (.85, .05), frameon = False)\n\n\nplt.subplot(3,1,2)\ng1 = sns.swarmplot(x='TotalWorkingYears', y= 'MonthlyIncome', \n                    dodge=True, hue='Attrition', data=df_train)\nax3 = g1.twinx()\nattr_temp = get_ratio(df_train, 'TotalWorkingYears')\ngg = sns.lineplot(x='TotalWorkingYears', y= 'ratio', ax=ax3, lw=3, markers='o',\n             label=\"Attrition %\", color='black',\n             data=attr_temp)\ngg.legend( loc = (.85, .05), frameon = False)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=0)\ng1.set_title(\"Total Working Years Distribution by Attrition and Age\", fontsize=22)\ng1.set_xlabel(\"Total Working Years\", fontsize=18)\ng1.set_ylabel(\"Age Distribution\", fontsize=18)\n\n\nplt.subplot(3,1,3)\ng2 = sns.swarmplot(x='HourlyRate', y='TotalWorkingYears', hue='Attrition', data=df_train)\ng2.set_title(\"Hourly Rate Distribution by Attrition and Age\", fontsize=22)\ng2.set_xlabel(\"Hourly Rate\", fontsize=18)\ng2.set_ylabel(\"Age Distribution\", fontsize=18)\ng2.set_xticklabels(g2.get_xticklabels(),rotation=0)\nax4 = g2.twinx()\nattr_temp = get_ratio(df_train, 'HourlyRate')\ngg = sns.lineplot(x='HourlyRate', y= 'ratio', ax=ax4, lw=3, markers='o',\n                  label=\"Attrition %\", color='black',\n                  data=attr_temp)\n\ngg.legend( loc = (.85, .05), frameon = False)\n\nplt.subplots_adjust(hspace = 0.4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very cool... This features are very meaningful and shows some patterns in Attrition; <br>\nWe can see a clear pattern in HourlyRate. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Monthly Income x Age by Attrition"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(14,7))\n\nax= sns.scatterplot(x='MonthlyIncome', y='Age', data=df_train, hue='Attrition', \n                    alpha=0.8, size=df_train['NumCompaniesWorked'])\nax.set_title(\"Age distribution by Monthly Income separated by Attrition\", fontsize=22)\nax.set_xlabel(\"Monthly Income\", fontsize=18)\nax.set_ylabel(\"Age Distribution\", fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection and Preprocessing"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def LongDisWL1(data) : \n    if  data['DistanceFromHome'] > 11 and data['WorkLifeBalance'] == 1 :\n        return 1\n    else : \n        return 0\n    \ndef LongDisJobS1(data) : \n    if  data['DistanceFromHome'] > 11 and data['JobSatisfaction'] == 1 :\n        return 1\n    else : \n        return 0\n    \ndef LongDisJL1(data) : \n    if  data['DistanceFromHome'] > 11 and data['JobLevel'] == 1 :\n        return 1\n    else : \n        return 0\n    \ndef ShortDisNotSingle(data) : \n    if  data['MaritalStatus'] != 'Single' and data['DistanceFromHome'] < 5:\n        return 1\n    else : \n        return 0\n    \ndef LongDisSingle(data) : \n    if  data['MaritalStatus'] == 'Single' and data['DistanceFromHome'] > 11:\n        return 1\n    else : \n        return 0\n    \ndef Engaged(data) : \n    if data['Age'] > 35 and data['MaritalStatus'] != 'Single':\n        return 1\n    else : \n        return 0\n    \ndef YoungAndBadPaid(data) : \n    if data['Age'] < 35 and data['Age'] > 23 and (data['MonthlyIncome'] < 3500):\n        return 1\n    else : \n        return 0\n    \ndef YoungNeverEngaged(data) : \n    if data['Age'] < 24 and data['MaritalStatus'] == 'Single' :\n        return 1\n    else : \n        return 0    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## This features I get from the  amazing kernel of Vicent Lugat\n## https://www.kaggle.com/kernels/scriptcontent/10006574/notebook\n\ndf_train['sales_dep'] = [1 if val == 'Sales' else 0 for val in df_train['Department']]\ndf_train['JobInvolvCut'] = [1 if val < 2.5 else 0 for val in df_train['JobInvolvement']]\ndf_train['MiddleTraining'] = [1 if (val  >= 3 and val <= 6) else 0 for val in df_train['TrainingTimesLastYear']]\ndf_train['MoovingPeople'] = [1 if (val  >4) else 0 for val in df_train['NumCompaniesWorked']]\ndf_train['MiddleTraining'] = [1 if (val  >= 3 and val <= 6) else 0 for val in df_train['TrainingTimesLastYear']]\n\ndf_train['TotalSatisfaction_mean'] = (df_train['RelationshipSatisfaction'] \\\n                                      + df_train['EnvironmentSatisfaction'] \\\n                                      + df_train['JobSatisfaction'] \\\n                                      + df_train['JobInvolvement'] \\\n                                      + df_train['WorkLifeBalance']) / 5\n\ndf_train['NotSatif'] = [1 if val < 2.35 else 0 for val in df_train['TotalSatisfaction_mean']]\ndf_train['LongDisWL1'] = df_train.apply(lambda data:LongDisWL1(data) ,axis = 1)\ndf_train['LongDis'] = [1 if val > 11 else 0 for val in df_train['DistanceFromHome']]\ndf_train['LongDisJobS1'] = df_train.apply(lambda data: LongDisJobS1(data) ,axis = 1)\ndf_train['LongDisJL1'] = df_train.apply(lambda data:LongDisJL1(data) ,axis = 1)\ndf_train['ShortDisNotSingle'] = df_train.apply(lambda data:ShortDisNotSingle(data) ,axis = 1)\ndf_train['LongDisSingle'] = df_train.apply(lambda data:LongDisSingle(data) ,axis = 1)\ndf_train['Engaged'] = df_train.apply(lambda data:Engaged(data) ,axis = 1)\ndf_train['YoungAndBadPaid'] = df_train.apply(lambda data:YoungAndBadPaid(data) ,axis = 1)\ndf_train['YoungNeverEngaged'] = df_train.apply(lambda data:YoungNeverEngaged(data) ,axis = 1)\n\ndf_train['Time_in_each_comp'] = (df_train['Age'] - 20) / ((df_train)['NumCompaniesWorked'] + 1)\ndf_train['RelSatisf_mean'] = (df_train['RelationshipSatisfaction']  + df_train['EnvironmentSatisfaction']) / 2\ndf_train['JobSatisf_mean'] = (df_train['JobSatisfaction'] + df_train['JobInvolvement']) / 2\ndf_train['Income_Distance'] = df_train['MonthlyIncome'] / df_train['DistanceFromHome']\ndf_train['Hrate_Mrate'] = df_train['HourlyRate'] / df_train['MonthlyRate']\ndf_train['Stability'] = df_train['YearsInCurrentRole'] / df_train['YearsAtCompany']\ndf_train['Stability'].fillna((df_train['Stability'].mean()), inplace=True)\ndf_train['Income_YearsComp'] = df_train['MonthlyIncome'] / df_train['YearsAtCompany']\ndf_train['Income_YearsComp'] = df_train['Income_YearsComp'].replace(np.Inf, 0)\ndf_train['Fidelity'] = (df_train['NumCompaniesWorked']) / df_train['TotalWorkingYears']\ndf_train['Fidelity'] = df_train['Fidelity'].replace(np.Inf, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def attr_ratio(df, col):\n    attr = df.groupby([col, 'Attrition'])['EmployeeNumber'].nunique().unstack('Attrition').reset_index()\n    attr['ratio'] =  attr['Yes'] / (attr['No'] + attr['Yes'])\n    \n    return attr"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing "},{"metadata":{"trusted":true},"cell_type":"code","source":"#customer id col\nId_col = ['EmployeeNumber']\n\n#Target columns\ntarget_col = [\"Attrition\"]\n\n#categorical columns\ncat_cols = df_train.nunique()[df_train.nunique() <= 10].keys().tolist()\ncat_cols = [x for x in cat_cols if x not in target_col]\n\n#numerical columns\nnum_cols = [x for x in df_train.columns if x not in cat_cols + target_col + Id_col]\n\n#Binary columns with 2 values\nbin_cols = df_train.nunique()[df_train.nunique() == 2].keys().tolist()\n\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_binary_cols = {'Attrition':{'Yes':1, 'No':0}, \n                    'Gender':{'Female':0, 'Male':1}, \n                    'OverTime':{'Yes':1,'No':0}}\ndf_train.replace(dict_binary_cols, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_cats = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\ndf_train = dummies(df_train, nom_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finallt, lets look the correlation of df_train\nplt.figure(figsize=(20,15))\nplt.title('Correlation of Features for Train Set', fontsize=25)\nsns.heatmap(df_train.astype(float).corr(), vmax=1.0 )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.80\n\n# Absolute value correlation matrix\ncorr_matrix = df_train.corr().abs()\n\n# Getting the upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint(list(to_drop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(columns = to_drop)\nprint('Training shape: ', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(df_train.drop('Attrition', axis=1), df_train['Attrition'], test_size=.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the pipeline to compare classification algorithmns"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, \n                                                                       n_estimators=600))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- Cool!!! Logistic Regression has the best result to predict Attriction. I will use a GLM algo to get soem insights and see the feature importanfces."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regularization penalty space\npenalty = ['l1', 'l2']\n# Create regularization hyperparameter space\nC = np.logspace(0, 5, 25)\n\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp \nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\n\ndef objective(params):\n    clf = LogisticRegression(**params, solver='liblinear'\n    )\n    \n    score = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=StratifiedKFold()).mean()\n    print(\"Accuracy {:.8f} params {}\".format(-score, params))\n    return -score\n\nspace = {\n    'penalty': hp.choice('penalty', ['l1', 'l2']),\n    'C':  hp.choice('C', np.logspace(5, 10, 50))}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = {'C': 13894954.94373136, 'penalty': 'l2'}   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(**best, solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.fit(X_train, y_train, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing our model in a unseen data"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_val, logreg.predict(X_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's slightly better than the standard model that we used on Pipeline. "},{"metadata":{},"cell_type":"markdown","source":"Let's see the Auc and Confusion matrix to understand the classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['Yes', 'No']\nprint(classification_report(y_val, logreg.predict(X_val), target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class_names = df_train['Attrition'].unique()\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix: \")\n    else:\n        print('Confusion matrix, without normalization: ')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_val, logreg.predict(X_val), classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_val, logreg.predict(X_val), classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NOTE: I am working on this kernel, it's not finished yet.\nIf you liked, don't forget to votes up the kernel !!! =) "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}