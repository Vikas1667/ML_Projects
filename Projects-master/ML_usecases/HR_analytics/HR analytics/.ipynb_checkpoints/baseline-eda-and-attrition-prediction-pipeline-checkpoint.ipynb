{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WELCOME TO IBM HR ATTRITION EXPLORATION AND PREDICTION KERNEL\n",
    "NOTE: I'm not a native English Speaker, so sorry for any english mistakes\n",
    "\n",
    "The main objective is to explore the data and create a model to predict the Attrition of IBM workers.\n",
    "\n",
    "### The problem\n",
    "We will explore and try to predict the Attrition of IBM HR Analytics data. <br> \n",
    "\n",
    "What Is Attrition?<br>\n",
    "Attrition in business describes a gradual but deliberate reduction in staff numbers that occurs as employees retire or resign and are <b>not replaced</b>. The term is also sometimes used to describe the loss of customers or clients as they mature beyond a product or company's target market without being replaced by a younger generation.\n",
    "\n",
    "Important: Attrition is one way a company can decrease labor costs without the disruption of layoffs.\n",
    " \n",
    " \n",
    "### Questions\n",
    "I will start with some questions that maybe will help me in exploration:\n",
    "- What's the % of Attrition at HR IBM dataset?\n",
    "- What's the distribution of Ages?\n",
    "- What's the difference between Genders?\n",
    "- The years of experience is important to Attrition ?\n",
    "- The performance or job satisfaction distributions says anything about the Attrition?\n",
    "- People that live far from the job, are more propense to Attrition?\n",
    "- And many more questions that could help us to understand the data and get some insights.\n",
    "\n",
    "### After EDA:\n",
    "I will create a Pipeline to find the model that best fit the data;\n",
    "Also, I will create a Hyperopt model to find the best parameters to predict the Attrition of workers;\n",
    "______________________________________\n",
    "<br>\n",
    "- I hope you enjoy the Kernel. <br>\n",
    "- If you think that it is useful for you, please votes and give me your feedback =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nThe plotly.plotly module is deprecated,\nplease install the chart-studio package and use the\nchart_studio.plotly module instead. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-659b17bdeb3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Standard plotly imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\plotly\\plotly\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_plotly_future_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_chart_studio_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0m_chart_studio_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"plotly\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\_plotly_future_\\__init__.py\u001b[0m in \u001b[0;36m_chart_studio_error\u001b[1;34m(submodule)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_chart_studio_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m     45\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[1;33m}\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nThe plotly.plotly module is deprecated,\nplease install the chart-studio package and use the\nchart_studio.plotly module instead. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import stats \n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Standard plotly imports\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import cufflinks\n",
    "import cufflinks as cf\n",
    "\n",
    "# Using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "cufflinks.go_offline(connected=True)\n",
    "\n",
    "#Importing the auxiliar and preprocessing librarys \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "\n",
    "#Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def resumetable(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "    \n",
    "    for name in summary['Name'].value_counts().index:\n",
    "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_ratio(df, cat_col):\n",
    "    attr_temp = pd.DataFrame(df.groupby([cat_col, 'Attrition'])['EmployeeNumber'].count().unstack('Attrition').reset_index())\n",
    "    attr_temp['ratio'] = round(attr_temp['Yes'] / (attr_temp['Yes'] + attr_temp['No']) * 100,2)\n",
    "    attr_temp = attr_temp[[cat_col, 'ratio']]\n",
    "    \n",
    "    return attr_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the summary of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "summary = resumetable(df_train)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first summary of data, we can see that we haven't any missing value and 3 columns have constant values.<br>\n",
    "Before we continue, I will drop the constant features; <br>\n",
    "It's very cool to see that we have a lot of categorical features!!! So we could have some interesting insights <br>\n",
    "The shape of our data is (1470, 35) and the EmployeeNumber is the Id of our dataset. <br>\n",
    "Also is important to note that entropy measure the data disorder of the each feature and it shows the information contained in this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features with maximum 10 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "cat_cols = ['Over18', 'StandardHours', 'EmployeeCount', 'Gender', 'PerformanceRating', \n",
    "            'OverTime', 'MaritalStatus', 'Department', 'BusinessTravel', 'StockOptionLevel', \n",
    "            'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', \n",
    "            'RelationshipSatisfaction', 'WorkLifeBalance', 'Education', \n",
    "            'JobLevel', 'EducationField', 'TrainingTimesLastYear', \n",
    "            'JobRole', 'NumCompaniesWorked']\n",
    "\n",
    "# constant features\n",
    "const_list = ['EmployeeCount',  'Over18',  'StandardHours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(const_list,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the distribution of Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA TYPES: \")\n",
    "print(summary['dtypes'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nice, Now, we will start exploring the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will take a look on Attrition features that is our Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(\"The % distribution of Attrition features is: \")\n",
    "print(round(df_train['Attrition'].value_counts(normalize=True),2)*100)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "g = sns.countplot(df_train[\"Attrition\"], color='green')\n",
    "g.set_title(\"Attrition Distribution\", fontsize=22)\n",
    "g.set_ylabel('Count', fontsize=18)\n",
    "g.set_xlabel('Attrition True or False', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we have 16% of true values of our target. It's a imbalanced data but nothing so terrible. <br>\n",
    "Let's keep exploring the features to see wether we can get some insights about the IBM workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting categorical features\n",
    "- First, I will plot all features that has less than 11 values; I will do it, because categories with few values are easiest to explore.\n",
    "- I will drop the constant columns and the Attrition feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering the constant features and the target \n",
    "cat_cols = [col for col in cat_cols if col not in (const_list +['Attrition'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoricals by ATTRITION\n",
    "-  Just features with maximum 10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(\"UNDERSTANDING THE CATEGORICAL DISTRIBUTION BY THE TARGET (ATTRITION)\")\n",
    "print(\"NOTE: - It's a plot just about the columns with maximum 10 values.\")\n",
    "fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(18,35))\n",
    "fig.subplots_adjust(hspace=0.5, bottom=0)\n",
    "# fig.suptitle('BINARY FEATURES by the TARGET feature', fontsize=22)\n",
    "\n",
    "for ax, catplot in zip(axes.flatten(), cat_cols):\n",
    "        sns.countplot(x=catplot, data=df_train, hue='Attrition', ax=ax, )\n",
    "        ## GEting the ratio of Years with current manager just to test into graphs\n",
    "        ax.set_title(catplot.upper(), fontsize=18)\n",
    "        ax.set_ylabel('Count', fontsize=16)\n",
    "        ax.set_xlabel(f'{catplot} Values', fontsize=15)\n",
    "        ax.legend(title='Attrition', fontsize=12)\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!!! We can see that some features has a high chance to Attrition. <br>\n",
    "Some features that we can see that the Attrition has different patterns is: stockoptionlevel,  performancerating, overtime, department, jobinvolvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous and large categorical features\n",
    "- Now I will explore the features with more than 10 values and try get some insights using the Attrition to see the different patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# PercentSalaryHike, DistanceFromHome, Age\n",
    "print(f'Minimum age on dataset is {df_train[\"Age\"].min()} and the maximum age is {df_train[\"Age\"].max()}')\n",
    "plt.figure(figsize=(16,22))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "g = sns.distplot(df_train[df_train['Attrition'] == 'Yes']['Age'], label='Yes')\n",
    "g = sns.distplot(df_train[df_train['Attrition'] == 'No']['Age'], label=\"No\")\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=0)\n",
    "g.legend(title='Attrition')\n",
    "g.set_title(\"Age Distribution by Attrition\", fontsize=22)\n",
    "g.set_xlabel(\"Age Distribution\", fontsize=18)\n",
    "g.set_ylabel(\"Probability\", fontsize=18)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "g1 = sns.violinplot(x='DistanceFromHome', y='Age', hue='Attrition', \n",
    "                    split=True, data=df_train, size=3)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=0)\n",
    "g1.set_title(\"Distance From Home Distribution by Attrition and Age\", fontsize=22)\n",
    "g1.set_xlabel(\"Distance From Home\", fontsize=18)\n",
    "g1.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "g2 = sns.violinplot(x='PercentSalaryHike', y='Age',\n",
    "                    split=True, hue='Attrition',data=df_train)\n",
    "g2.set_xticklabels(g2.get_xticklabels(),rotation=0)\n",
    "g2.set_title(\"Percent Salary Hike Distribution by Attrition and Age\", fontsize=22)\n",
    "g2.set_xlabel(\"Percent Salary Hike\", fontsize=18)\n",
    "g2.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very insightful informations!! <br>\n",
    "Based on charts we can see that the attrition is more probable in yougest people;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medimum_feats = ['PercentSalaryHike', 'YearsSinceLastPromotion', 'YearsWithCurrManager', \n",
    "                 'YearsInCurrentRole', 'DistanceFromHome']\n",
    "                 \n",
    "big_feats = [ 'YearsAtCompany', 'TotalWorkingYears', 'Age','HourlyRate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,22))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "g = sns.violinplot(x='YearsInCurrentRole', y= 'Age', \n",
    "                   split=True, hue='Attrition', data=df_train)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=0)\n",
    "g.set_title(\"Years In Current Role by Attrition and Age\", fontsize=22)\n",
    "g.set_xlabel(\"Years In Current Role\", fontsize=18)\n",
    "g.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "g1 = sns.violinplot(x='YearsSinceLastPromotion', y= 'Age', \n",
    "                    split=True, hue='Attrition', data=df_train)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=0)\n",
    "g1.set_title(\"Years Since Last Promotion Distribution by Attrition and Age\", fontsize=22)\n",
    "g1.set_xlabel(\"Years since last Promotion\", fontsize=18)\n",
    "g1.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "g2 = sns.violinplot(x='YearsAtCompany', y= 'Age', \n",
    "                   split=True, hue='Attrition', data=df_train)\n",
    "g2.set_xticklabels(g2.get_xticklabels(),rotation=0)\n",
    "g2.set_title(\"Years At Company by Attrition and Age\", fontsize=22)\n",
    "g2.set_xlabel(\"Years In Current Role\", fontsize=18)\n",
    "g2.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. On Years at Company we can see that people with more than 12 years are less propense to leave the company.<br>\n",
    "The same pattern we can see on Years in current Role... After 15 years in the current role, we don't have siginificant number of Attrition; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will try some visuals with the Monthly Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The minimum value Income in dataset is {df_train['MonthlyIncome'].min()} and maximum {df_train['MonthlyIncome'].max()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monthly Income Quantiles Distribution: \")\n",
    "print(df_train['MonthlyIncome'].quantile([.01, .25, .5, .75, .99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_ratio(df, cat_col):\n",
    "    attr_temp = df.groupby([cat_col, 'Attrition'])['EmployeeNumber'].count().unstack('Attrition').reset_index()\n",
    "    attr_temp['ratio'] = round(attr_temp['Yes'] / (attr_temp['Yes'] + attr_temp['No']) * 100,2)\n",
    "    attr_temp = attr_temp[[cat_col, 'ratio']]\n",
    "    \n",
    "    return attr_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,22))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "\n",
    "g = sns.violinplot(x='YearsWithCurrManager', y= 'MonthlyIncome', \n",
    "                    hue='Attrition',data=df_train, split=True)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=0)\n",
    "g.set_title(\"Years With Current Manager Distribution by Attrition and Monthly Income\", fontsize=22)\n",
    "g.set_xlabel(\"Years with current Manager\", fontsize=18)\n",
    "g.set_ylabel(\"Monthly Income Distribution\", fontsize=18)\n",
    "ax2 = g.twinx()\n",
    "attr_temp = get_ratio(df_train, 'YearsWithCurrManager')\n",
    "gg = sns.lineplot(x='YearsWithCurrManager', y= 'ratio', ax=ax2, lw=3, markers='o',\n",
    "             label=\"Attrition %\", color='black',\n",
    "             data=attr_temp)\n",
    "gg.legend( loc = (.85, .05), frameon = False)\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "g1 = sns.swarmplot(x='TotalWorkingYears', y= 'MonthlyIncome', \n",
    "                    dodge=True, hue='Attrition', data=df_train)\n",
    "ax3 = g1.twinx()\n",
    "attr_temp = get_ratio(df_train, 'TotalWorkingYears')\n",
    "gg = sns.lineplot(x='TotalWorkingYears', y= 'ratio', ax=ax3, lw=3, markers='o',\n",
    "             label=\"Attrition %\", color='black',\n",
    "             data=attr_temp)\n",
    "gg.legend( loc = (.85, .05), frameon = False)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=0)\n",
    "g1.set_title(\"Total Working Years Distribution by Attrition and Age\", fontsize=22)\n",
    "g1.set_xlabel(\"Total Working Years\", fontsize=18)\n",
    "g1.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "g2 = sns.swarmplot(x='HourlyRate', y='TotalWorkingYears', hue='Attrition', data=df_train)\n",
    "g2.set_title(\"Hourly Rate Distribution by Attrition and Age\", fontsize=22)\n",
    "g2.set_xlabel(\"Hourly Rate\", fontsize=18)\n",
    "g2.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "g2.set_xticklabels(g2.get_xticklabels(),rotation=0)\n",
    "ax4 = g2.twinx()\n",
    "attr_temp = get_ratio(df_train, 'HourlyRate')\n",
    "gg = sns.lineplot(x='HourlyRate', y= 'ratio', ax=ax4, lw=3, markers='o',\n",
    "                  label=\"Attrition %\", color='black',\n",
    "                  data=attr_temp)\n",
    "\n",
    "gg.legend( loc = (.85, .05), frameon = False)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool... This features are very meaningful and shows some patterns in Attrition; <br>\n",
    "We can see a clear pattern in HourlyRate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Income x Age by Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "ax= sns.scatterplot(x='MonthlyIncome', y='Age', data=df_train, hue='Attrition', \n",
    "                    alpha=0.8, size=df_train['NumCompaniesWorked'])\n",
    "ax.set_title(\"Age distribution by Monthly Income separated by Attrition\", fontsize=22)\n",
    "ax.set_xlabel(\"Monthly Income\", fontsize=18)\n",
    "ax.set_ylabel(\"Age Distribution\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def LongDisWL1(data) : \n",
    "    if  data['DistanceFromHome'] > 11 and data['WorkLifeBalance'] == 1 :\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def LongDisJobS1(data) : \n",
    "    if  data['DistanceFromHome'] > 11 and data['JobSatisfaction'] == 1 :\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def LongDisJL1(data) : \n",
    "    if  data['DistanceFromHome'] > 11 and data['JobLevel'] == 1 :\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def ShortDisNotSingle(data) : \n",
    "    if  data['MaritalStatus'] != 'Single' and data['DistanceFromHome'] < 5:\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def LongDisSingle(data) : \n",
    "    if  data['MaritalStatus'] == 'Single' and data['DistanceFromHome'] > 11:\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def Engaged(data) : \n",
    "    if data['Age'] > 35 and data['MaritalStatus'] != 'Single':\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def YoungAndBadPaid(data) : \n",
    "    if data['Age'] < 35 and data['Age'] > 23 and (data['MonthlyIncome'] < 3500):\n",
    "        return 1\n",
    "    else : \n",
    "        return 0\n",
    "    \n",
    "def YoungNeverEngaged(data) : \n",
    "    if data['Age'] < 24 and data['MaritalStatus'] == 'Single' :\n",
    "        return 1\n",
    "    else : \n",
    "        return 0    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "## This features I get from the  amazing kernel of Vicent Lugat\n",
    "## https://www.kaggle.com/kernels/scriptcontent/10006574/notebook\n",
    "\n",
    "df_train['sales_dep'] = [1 if val == 'Sales' else 0 for val in df_train['Department']]\n",
    "df_train['JobInvolvCut'] = [1 if val < 2.5 else 0 for val in df_train['JobInvolvement']]\n",
    "df_train['MiddleTraining'] = [1 if (val  >= 3 and val <= 6) else 0 for val in df_train['TrainingTimesLastYear']]\n",
    "df_train['MoovingPeople'] = [1 if (val  >4) else 0 for val in df_train['NumCompaniesWorked']]\n",
    "df_train['MiddleTraining'] = [1 if (val  >= 3 and val <= 6) else 0 for val in df_train['TrainingTimesLastYear']]\n",
    "\n",
    "df_train['TotalSatisfaction_mean'] = (df_train['RelationshipSatisfaction'] \\\n",
    "                                      + df_train['EnvironmentSatisfaction'] \\\n",
    "                                      + df_train['JobSatisfaction'] \\\n",
    "                                      + df_train['JobInvolvement'] \\\n",
    "                                      + df_train['WorkLifeBalance']) / 5\n",
    "\n",
    "df_train['NotSatif'] = [1 if val < 2.35 else 0 for val in df_train['TotalSatisfaction_mean']]\n",
    "df_train['LongDisWL1'] = df_train.apply(lambda data:LongDisWL1(data) ,axis = 1)\n",
    "df_train['LongDis'] = [1 if val > 11 else 0 for val in df_train['DistanceFromHome']]\n",
    "df_train['LongDisJobS1'] = df_train.apply(lambda data: LongDisJobS1(data) ,axis = 1)\n",
    "df_train['LongDisJL1'] = df_train.apply(lambda data:LongDisJL1(data) ,axis = 1)\n",
    "df_train['ShortDisNotSingle'] = df_train.apply(lambda data:ShortDisNotSingle(data) ,axis = 1)\n",
    "df_train['LongDisSingle'] = df_train.apply(lambda data:LongDisSingle(data) ,axis = 1)\n",
    "df_train['Engaged'] = df_train.apply(lambda data:Engaged(data) ,axis = 1)\n",
    "df_train['YoungAndBadPaid'] = df_train.apply(lambda data:YoungAndBadPaid(data) ,axis = 1)\n",
    "df_train['YoungNeverEngaged'] = df_train.apply(lambda data:YoungNeverEngaged(data) ,axis = 1)\n",
    "\n",
    "df_train['Time_in_each_comp'] = (df_train['Age'] - 20) / ((df_train)['NumCompaniesWorked'] + 1)\n",
    "df_train['RelSatisf_mean'] = (df_train['RelationshipSatisfaction']  + df_train['EnvironmentSatisfaction']) / 2\n",
    "df_train['JobSatisf_mean'] = (df_train['JobSatisfaction'] + df_train['JobInvolvement']) / 2\n",
    "df_train['Income_Distance'] = df_train['MonthlyIncome'] / df_train['DistanceFromHome']\n",
    "df_train['Hrate_Mrate'] = df_train['HourlyRate'] / df_train['MonthlyRate']\n",
    "df_train['Stability'] = df_train['YearsInCurrentRole'] / df_train['YearsAtCompany']\n",
    "df_train['Stability'].fillna((df_train['Stability'].mean()), inplace=True)\n",
    "df_train['Income_YearsComp'] = df_train['MonthlyIncome'] / df_train['YearsAtCompany']\n",
    "df_train['Income_YearsComp'] = df_train['Income_YearsComp'].replace(np.Inf, 0)\n",
    "df_train['Fidelity'] = (df_train['NumCompaniesWorked']) / df_train['TotalWorkingYears']\n",
    "df_train['Fidelity'] = df_train['Fidelity'].replace(np.Inf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def attr_ratio(df, col):\n",
    "    attr = df.groupby([col, 'Attrition'])['EmployeeNumber'].nunique().unstack('Attrition').reset_index()\n",
    "    attr['ratio'] =  attr['Yes'] / (attr['No'] + attr['Yes'])\n",
    "    \n",
    "    return attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer id col\n",
    "Id_col = ['EmployeeNumber']\n",
    "\n",
    "#Target columns\n",
    "target_col = [\"Attrition\"]\n",
    "\n",
    "#categorical columns\n",
    "cat_cols = df_train.nunique()[df_train.nunique() <= 10].keys().tolist()\n",
    "cat_cols = [x for x in cat_cols if x not in target_col]\n",
    "\n",
    "#numerical columns\n",
    "num_cols = [x for x in df_train.columns if x not in cat_cols + target_col + Id_col]\n",
    "\n",
    "#Binary columns with 2 values\n",
    "bin_cols = df_train.nunique()[df_train.nunique() == 2].keys().tolist()\n",
    "\n",
    "#Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_binary_cols = {'Attrition':{'Yes':1, 'No':0}, \n",
    "                    'Gender':{'Female':0, 'Male':1}, \n",
    "                    'OverTime':{'Yes':1,'No':0}}\n",
    "df_train.replace(dict_binary_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_cats = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "df_train = dummies(df_train, nom_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finallt, lets look the correlation of df_train\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title('Correlation of Features for Train Set', fontsize=25)\n",
    "sns.heatmap(df_train.astype(float).corr(), vmax=1.0 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.80\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = df_train.corr().abs()\n",
    "\n",
    "# Getting the upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))\n",
    "print(list(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns = to_drop)\n",
    "print('Training shape: ', df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train.drop('Attrition', axis=1), df_train['Attrition'], test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline to compare classification algorithmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "seed = 3\n",
    "\n",
    "clfs.append((\"LogReg\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"LogReg\", LogisticRegression())])))\n",
    "\n",
    "clfs.append((\"XGBClassifier\",\n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"XGB\", XGBClassifier())]))) \n",
    "clfs.append((\"KNN\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"KNN\", KNeighborsClassifier())]))) \n",
    "\n",
    "clfs.append((\"DecisionTreeClassifier\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n",
    "\n",
    "clfs.append((\"RandomForestClassifier\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"RandomForest\", RandomForestClassifier())]))) \n",
    "\n",
    "clfs.append((\"GradientBoostingClassifier\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, \n",
    "                                                                       n_estimators=600))]))) \n",
    "\n",
    "clfs.append((\"RidgeClassifier\", \n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"RidgeClassifier\", RidgeClassifier())])))\n",
    "\n",
    "clfs.append((\"BaggingRidgeClassifier\",\n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"BaggingClassifier\", BaggingClassifier())])))\n",
    "\n",
    "clfs.append((\"ExtraTreesClassifier\",\n",
    "             Pipeline([(\"Scaler\", StandardScaler()),\n",
    "                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n",
    "\n",
    "#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\n",
    "scoring = 'accuracy'\n",
    "n_folds = 10\n",
    "\n",
    "results, names  = [], [] \n",
    "\n",
    "for name, model  in clfs:\n",
    "    kfold = KFold(n_splits=n_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, \n",
    "                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n",
    "    names.append(name)\n",
    "    results.append(cv_results)    \n",
    "    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  \n",
    "                               cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "fig.suptitle('Classifier Algorithm Comparison', fontsize=22)\n",
    "ax = fig.add_subplot(111)\n",
    "sns.boxplot(x=names, y=results)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_xlabel(\"Algorithmn\", fontsize=20)\n",
    "ax.set_ylabel(\"Accuracy of Models\", fontsize=18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cool!!! Logistic Regression has the best result to predict Attriction. I will use a GLM algo to get soem insights and see the feature importanfces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 5, 25)\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp \n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def objective(params):\n",
    "    clf = LogisticRegression(**params, solver='liblinear'\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=StratifiedKFold()).mean()\n",
    "    print(\"Accuracy {:.8f} params {}\".format(-score, params))\n",
    "    return -score\n",
    "\n",
    "space = {\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'C':  hp.choice('C', np.logspace(5, 10, 50))}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'C': 13894954.94373136, 'penalty': 'l2'}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(**best, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model in a unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val, logreg.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slightly better than the standard model that we used on Pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Auc and Confusion matrix to understand the classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Yes', 'No']\n",
    "print(classification_report(y_val, logreg.predict(X_val), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class_names = df_train['Attrition'].unique()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix: \")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization: ')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_val, logreg.predict(X_val), classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_val, logreg.predict(X_val), classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: I am working on this kernel, it's not finished yet.\n",
    "If you liked, don't forget to votes up the kernel !!! =) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
