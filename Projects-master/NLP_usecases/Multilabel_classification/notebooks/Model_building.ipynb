{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_txt_data=pd.read_csv('./dataset/train.csv')\n",
    "# test_txt_data=pd.read_csv('./dataset/test.csv')\n",
    "# string.punctuation\n",
    "# train_txt_data['comment_text']=train_txt_data['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\") \n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# train_txt_data['clean']=train_txt_data['comment_text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\") \n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "     \n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# train_txt_data['clean']=train_txt_data['comment_text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_txt_data['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('./Process_Train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean</th>\n",
       "      <th>comment_text_tokens</th>\n",
       "      <th>comment_text_sent_clean</th>\n",
       "      <th>Lemmatized tokens</th>\n",
       "      <th>Lemmatized text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>['explanation', 'edits', 'made', 'username', '...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>['explanation', 'edits', 'made', 'username', '...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>['daww', 'matches', 'background', 'colour', 'i...</td>\n",
       "      <td>daww matches background colour im seemingly st...</td>\n",
       "      <td>['daww', 'match', 'background', 'colour', 'im'...</td>\n",
       "      <td>daww match background colour im seemingly stuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>['hey', 'man', 'im', 'really', 'trying', 'edit...</td>\n",
       "      <td>hey man im really trying edit war guy constant...</td>\n",
       "      <td>['hey', 'man', 'im', 'really', 'trying', 'edit...</td>\n",
       "      <td>hey man im really trying edit war guy constant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore cant make any real suggestions on impro...</td>\n",
       "      <td>['cant', 'make', 'real', 'suggestions', 'impro...</td>\n",
       "      <td>cant make real suggestions improvement wondere...</td>\n",
       "      <td>['cant', 'make', 'real', 'suggestion', 'improv...</td>\n",
       "      <td>cant make real suggestion improvement wondered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::and for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>['second', 'time', 'asking', 'view', 'complete...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>['second', 'time', 'asking', 'view', 'complete...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n",
       "      <td>['ashamed', 'horrible', 'thing', 'put', 'talk'...</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>['ashamed', 'horrible', 'thing', 'put', 'talk'...</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer \\n\\numm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer \\n\\numm theres no actual article for p...</td>\n",
       "      <td>['spitzer', 'umm', 'theres', 'actual', 'articl...</td>\n",
       "      <td>spitzer umm theres actual article prostitution...</td>\n",
       "      <td>['spitzer', 'umm', 'there', 'actual', 'article...</td>\n",
       "      <td>spitzer umm there actual article prostitution ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>['looks', 'like', 'actually', 'put', 'speedy',...</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>['look', 'like', 'actually', 'put', 'speedy', ...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nand ... i really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nand  really dont think you understand came h...</td>\n",
       "      <td>['really', 'dont', 'think', 'understand', 'cam...</td>\n",
       "      <td>really dont think understand came idea bad rig...</td>\n",
       "      <td>['really', 'dont', 'think', 'understand', 'cam...</td>\n",
       "      <td>really dont think understand came idea bad rig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation\\nwhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  d'aww! he matches this background colour i'm s...   \n",
       "2       000113f07ec002fd  hey man, i'm really not trying to edit war. it...   \n",
       "3       0001b41b1c6bb37e  \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  you, sir, are my hero. any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::and for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  you should be ashamed of yourself \\n\\nthat is ...   \n",
       "159568  ffee36eab5c267c9  spitzer \\n\\numm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  and it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nand ... i really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                                    clean  \\\n",
       "0       explanation\\nwhy the edits made under my usern...   \n",
       "1       daww he matches this background colour im seem...   \n",
       "2       hey man im really not trying to edit war its j...   \n",
       "3       \\nmore cant make any real suggestions on impro...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159566  and for the second time of asking when your vi...   \n",
       "159567  you should be ashamed of yourself \\n\\nthat is ...   \n",
       "159568  spitzer \\n\\numm theres no actual article for p...   \n",
       "159569  and it looks like it was actually you who put ...   \n",
       "159570  \\nand  really dont think you understand came h...   \n",
       "\n",
       "                                      comment_text_tokens  \\\n",
       "0       ['explanation', 'edits', 'made', 'username', '...   \n",
       "1       ['daww', 'matches', 'background', 'colour', 'i...   \n",
       "2       ['hey', 'man', 'im', 'really', 'trying', 'edit...   \n",
       "3       ['cant', 'make', 'real', 'suggestions', 'impro...   \n",
       "4       ['sir', 'hero', 'chance', 'remember', 'page', ...   \n",
       "...                                                   ...   \n",
       "159566  ['second', 'time', 'asking', 'view', 'complete...   \n",
       "159567  ['ashamed', 'horrible', 'thing', 'put', 'talk'...   \n",
       "159568  ['spitzer', 'umm', 'theres', 'actual', 'articl...   \n",
       "159569  ['looks', 'like', 'actually', 'put', 'speedy',...   \n",
       "159570  ['really', 'dont', 'think', 'understand', 'cam...   \n",
       "\n",
       "                                  comment_text_sent_clean  \\\n",
       "0       explanation edits made username hardcore metal...   \n",
       "1       daww matches background colour im seemingly st...   \n",
       "2       hey man im really trying edit war guy constant...   \n",
       "3       cant make real suggestions improvement wondere...   \n",
       "4                     sir hero chance remember page thats   \n",
       "...                                                   ...   \n",
       "159566  second time asking view completely contradicts...   \n",
       "159567               ashamed horrible thing put talk page   \n",
       "159568  spitzer umm theres actual article prostitution...   \n",
       "159569  looks like actually put speedy first version d...   \n",
       "159570  really dont think understand came idea bad rig...   \n",
       "\n",
       "                                        Lemmatized tokens  \\\n",
       "0       ['explanation', 'edits', 'made', 'username', '...   \n",
       "1       ['daww', 'match', 'background', 'colour', 'im'...   \n",
       "2       ['hey', 'man', 'im', 'really', 'trying', 'edit...   \n",
       "3       ['cant', 'make', 'real', 'suggestion', 'improv...   \n",
       "4       ['sir', 'hero', 'chance', 'remember', 'page', ...   \n",
       "...                                                   ...   \n",
       "159566  ['second', 'time', 'asking', 'view', 'complete...   \n",
       "159567  ['ashamed', 'horrible', 'thing', 'put', 'talk'...   \n",
       "159568  ['spitzer', 'umm', 'there', 'actual', 'article...   \n",
       "159569  ['look', 'like', 'actually', 'put', 'speedy', ...   \n",
       "159570  ['really', 'dont', 'think', 'understand', 'cam...   \n",
       "\n",
       "                                          Lemmatized text  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww match background colour im seemingly stuc...  \n",
       "2       hey man im really trying edit war guy constant...  \n",
       "3       cant make real suggestion improvement wondered...  \n",
       "4                     sir hero chance remember page thats  \n",
       "...                                                   ...  \n",
       "159566  second time asking view completely contradicts...  \n",
       "159567               ashamed horrible thing put talk page  \n",
       "159568  spitzer umm there actual article prostitution ...  \n",
       "159569  look like actually put speedy first version de...  \n",
       "159570  really dont think understand came idea bad rig...  \n",
       "\n",
       "[159571 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean</th>\n",
       "      <th>comment_text_tokens</th>\n",
       "      <th>comment_text_sent_clean</th>\n",
       "      <th>Lemmatized tokens</th>\n",
       "      <th>Lemmatized text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>05ac7a7a83e4c63a</td>\n",
       "      <td>no, it doesn´t.80.228.65.162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no it doesn t</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>067638a445ccd93b</td>\n",
       "      <td>here, here and here.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>here here and here</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>0aa6f3529219b37e</td>\n",
       "      <td>from here\\n\\nfrom here 160.80.2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>from here\\n\\nfrom here</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>0bed2196c873636d</td>\n",
       "      <td>1993\\n\\n1994\\n\\n1995\\n\\n1996\\n\\n1997\\n\\n1998\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>1089bc2866ce9379</td>\n",
       "      <td>http://www.imdb.com/name/nm2551199/filmoseries...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148865</th>\n",
       "      <td>5352b339650c4138</td>\n",
       "      <td>she did 76.122.79.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>she did</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151379</th>\n",
       "      <td>7c1a57063b5c14d5</td>\n",
       "      <td>10 - 2010 04 08 to 2010 05 12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>to</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152817</th>\n",
       "      <td>92f864a4cf6d7ee3</td>\n",
       "      <td>same for this 166.137.240.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>same for this</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153315</th>\n",
       "      <td>9af7112c4d554edb</td>\n",
       "      <td>which is over 9000 over 9000 over 9000 over 90...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>which is over  over  over  over  over  over  o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153616</th>\n",
       "      <td>9fe2529bbf1aa64a</td>\n",
       "      <td>http://en.wikipedia.org/w/index.php?title=cras...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "2098    05ac7a7a83e4c63a                       no, it doesn´t.80.228.65.162   \n",
       "2407    067638a445ccd93b                               here, here and here.   \n",
       "3990    0aa6f3529219b37e                  from here\\n\\nfrom here 160.80.2.8   \n",
       "4482    0bed2196c873636d  1993\\n\\n1994\\n\\n1995\\n\\n1996\\n\\n1997\\n\\n1998\\n...   \n",
       "6193    1089bc2866ce9379  http://www.imdb.com/name/nm2551199/filmoseries...   \n",
       "...                  ...                                                ...   \n",
       "148865  5352b339650c4138                               she did 76.122.79.82   \n",
       "151379  7c1a57063b5c14d5                      10 - 2010 04 08 to 2010 05 12   \n",
       "152817  92f864a4cf6d7ee3                       same for this 166.137.240.20   \n",
       "153315  9af7112c4d554edb  which is over 9000 over 9000 over 9000 over 90...   \n",
       "153616  9fe2529bbf1aa64a  http://en.wikipedia.org/w/index.php?title=cras...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "2098        0             0        0       0       0              0   \n",
       "2407        0             0        0       0       0              0   \n",
       "3990        0             0        0       0       0              0   \n",
       "4482        0             0        0       0       0              0   \n",
       "6193        0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "148865      0             0        0       0       0              0   \n",
       "151379      0             0        0       0       0              0   \n",
       "152817      0             0        0       0       0              0   \n",
       "153315      0             0        0       0       0              0   \n",
       "153616      0             0        0       0       0              0   \n",
       "\n",
       "                                                    clean comment_text_tokens  \\\n",
       "2098                                        no it doesn t                  []   \n",
       "2407                                   here here and here                  []   \n",
       "3990                              from here\\n\\nfrom here                   []   \n",
       "4482             \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                  []   \n",
       "6193                                                                       []   \n",
       "...                                                   ...                 ...   \n",
       "148865                                           she did                   []   \n",
       "151379                                              to                     []   \n",
       "152817                                     same for this                   []   \n",
       "153315  which is over  over  over  over  over  over  o...                  []   \n",
       "153616                                                                     []   \n",
       "\n",
       "       comment_text_sent_clean Lemmatized tokens Lemmatized text  \n",
       "2098                       NaN                []             NaN  \n",
       "2407                       NaN                []             NaN  \n",
       "3990                       NaN                []             NaN  \n",
       "4482                       NaN                []             NaN  \n",
       "6193                       NaN                []             NaN  \n",
       "...                        ...               ...             ...  \n",
       "148865                     NaN                []             NaN  \n",
       "151379                     NaN                []             NaN  \n",
       "152817                     NaN                []             NaN  \n",
       "153315                     NaN                []             NaN  \n",
       "153616                     NaN                []             NaN  \n",
       "\n",
       "[67 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['comment_text_sent_clean'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[train_data['comment_text_sent_clean'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[train_data['comment_text_sent_clean'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = get_vectorize_layer(\n",
    "    train_data.comment_text_sent_clean.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vectorize_layer\n",
    "# train_data.comment_text_sent_clean.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode(train_data.comment_text_sent_clean.values)  # encode reviews with vectorizer\n",
    "# y_train = train_df.sentiment.values\n",
    "# train_classifier_ds = (\n",
    "#     tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "#     .shuffle(1000)\n",
    "#     .batch(config.BATCH_SIZE)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 521,   45,   48, ...,    0,    0,    0],\n",
       "       [   1, 2318, 1267, ...,    0,    0,    0],\n",
       "       [ 311,  322,   14, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1, 7129,  270, ...,    0,    0,    0],\n",
       "       [ 415,    9,  109, ...,    0,    0,    0],\n",
       "       [  51,   10,   13, ...,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159504 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159504 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256) dtype=int64 (created by layer 'input_3')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=keras.Input((config.MAX_LEN,),dtype=tf.int64)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256, 128) dtype=float32 (created by layer 'word_embedding')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding=layers.Embedding(config.VOCAB_SIZE,config.EMBED_DIM,name='word_embedding')(inputs)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
