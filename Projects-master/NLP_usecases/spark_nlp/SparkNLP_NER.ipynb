{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AImonster:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x220fb762b08>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.3.2\n",
      "Apache Spark version:  3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AImonster:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x220fb762b08>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline\n",
    "If you have any trouble using online pipelines or models in your environment (maybe itâ€™s air-gapped), you can directly download them for offline use.\n",
    "\n",
    "After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster):\n",
    "\n",
    "val advancedPipeline = PipelineModel.load(\"/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/\")\n",
    "// To use the loaded Pipeline for prediction\n",
    "advancedPipeline.transform(predictionDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_768 download started this may take some time.\n",
      "Approximate size to download 139.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# First extract the prerequisites for the NerDLApproach\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Then the training can start\n",
    "nerTagger = NerDLApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setOutputCol(\"ner\") \\\n",
    "    .setMaxEpochs(1) \\\n",
    "    .setRandomSeed(0) \\\n",
    "    .setVerbose(0)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    sentence,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    nerTagger\n",
    "])\n",
    "\n",
    "# We use the text and labels from the CoNLL dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognize_entities_dl download started this may take some time.\n",
      "Approx size to download 160.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "#create or get Spark Session\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "sparknlp.version()\n",
    "spark.version\n",
    "\n",
    "#download, load and annotate a text by pre-trained pipeline\n",
    "\n",
    "pipeline = PretrainedPipeline('recognize_entities_dl', 'en')\n",
    "result = pipeline.annotate('The Mona Lisa is a 16th century oil painting created by Leonardo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['Mona Lisa', 'Leonardo'],\n",
       " 'document': ['The Mona Lisa is a 16th century oil painting created by Leonardo'],\n",
       " 'token': ['The',\n",
       "  'Mona',\n",
       "  'Lisa',\n",
       "  'is',\n",
       "  'a',\n",
       "  '16th',\n",
       "  'century',\n",
       "  'oil',\n",
       "  'painting',\n",
       "  'created',\n",
       "  'by',\n",
       "  'Leonardo'],\n",
       " 'ner': ['O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER'],\n",
       " 'embeddings': ['The',\n",
       "  'Mona',\n",
       "  'Lisa',\n",
       "  'is',\n",
       "  'a',\n",
       "  '16th',\n",
       "  'century',\n",
       "  'oil',\n",
       "  'painting',\n",
       "  'created',\n",
       "  'by',\n",
       "  'Leonardo'],\n",
       " 'sentence': ['The Mona Lisa is a 16th century oil painting created by Leonardo']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|keyword     |score            |\n",
      "+------------+-----------------+\n",
      "|pig isolates|0.594374093018378|\n",
      "|pig isolates|0.594374093018378|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "token = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setContextChars([\"(\", \"]\", \"?\", \"!\", \".\", \",\"])\n",
    "\n",
    "keywords = YakeKeywordExtraction() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"keywords\") \\\n",
    "    .setThreshold(0.6) \\\n",
    "    .setMinNGrams(2) \\\n",
    "    .setNKeywords(10)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    token,\n",
    "    keywords\n",
    "])\n",
    "\n",
    "data = spark.createDataFrame([[\n",
    "    \"contamination of meat with antimicrobial-resistant bacteria represents a major public health threat worldwide. in this study\\, we determined the antimicrobial resistance profiles and resistance trends of staphylococcus aureus isolated from major food animal carcasses (408 cattle\\, 1196 pig\\, and 1312 chicken carcass isolates) in korea from 2010 to 2018. approximately 75%\\, 92%\\, and 77% of cattle\\, pig\\, and chicken carcass isolates\\, respectively\\, were resistant to at least one antimicrobial agent. resistance to penicillin (62.1%) was the highest\\, followed by resistance to tetracycline (42.1%) and erythromycin (28.2%). about 30% of pig and chicken isolates were resistant to ciprofloxacin. we observed linezolid resistance only in pig isolates (2.3%). however\\, all s. aureus isolates were sensitive to rifampin and vancomycin. we noted an increasing but fluctuating trend of kanamycin and penicillin resistance in cattle isolates. similarly\\, the chloramphenicol\\, ciprofloxacin\\, tetracycline\\, and trimethoprim resistance rates were increased but fluctuated through time in pig isolates. methicillin-resistant s. aureus (mrsa) accounted for 5%\\, 8%\\, and 9% of the cattle\\, pig\\, and chicken isolates\\, respectively. the mrsa strains exhibited significantly high resistance rates to most of the tested antimicrobials\\, including ciprofloxacin\\, erythromycin\\, and tetracycline compared with methicillin-susceptible s. aureus (mssa) strains. notably\\, a relatively high percentage of mrsa strains (5.2%) recovered from pig carcasses were resistant to linezolid compared with mssa strains (2.1%). in addition\\, almost 37% of the isolates were multi-drug resistant. s. aureus isolates recovered from major food animal carcasses in korea exhibited resistance to clinically important antimicrobials\\, posing a public health risk.\"\n",
    "]]).toDF(\"text\")\n",
    "result = pipeline.fit(data).transform(data)\n",
    "\n",
    "# combine the result and score (contained in keywords.metadata)\n",
    "scores = result \\\n",
    "    .selectExpr(\"explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples\") \\\n",
    "    .selectExpr(\"resultTuples['0'] as keyword\", \"resultTuples['1'].score as score\")\n",
    "\n",
    "# Order ascending, as lower scores means higher importance\n",
    "scores.orderBy(\"score\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
