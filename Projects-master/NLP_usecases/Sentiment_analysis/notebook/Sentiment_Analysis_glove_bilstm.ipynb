{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tr3CMgRv1N</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wc81vGp8qZ</td>\n",
       "      <td>A Look at the Health of the Chinese Economy</td>\n",
       "      <td>Tim Haywood, investment director business-unit...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2008-09-20 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>-0.156386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zNGH03CrZH</td>\n",
       "      <td>Nouriel Roubini: Global Economy Not Back to 2008</td>\n",
       "      <td>Nouriel Roubini, NYU professor and chairman at...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.425210</td>\n",
       "      <td>0.139754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3sM1H0W8ts</td>\n",
       "      <td>Finland GDP Expands In Q4</td>\n",
       "      <td>Finland's economy expanded marginally in the t...</td>\n",
       "      <td>RTT News</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:06:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wUbnxgvqaZ</td>\n",
       "      <td>Tourism, govt spending buoys Thai economy in J...</td>\n",
       "      <td>Tourism and public spending continued to boost...</td>\n",
       "      <td>The Nation - Thailand&amp;#39;s English news</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:11:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IDLink                                              Title  \\\n",
       "0  Tr3CMgRv1N   Obama Lays Wreath at Arlington National Cemetery   \n",
       "1  Wc81vGp8qZ        A Look at the Health of the Chinese Economy   \n",
       "2  zNGH03CrZH   Nouriel Roubini: Global Economy Not Back to 2008   \n",
       "3  3sM1H0W8ts                          Finland GDP Expands In Q4   \n",
       "4  wUbnxgvqaZ  Tourism, govt spending buoys Thai economy in J...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...   \n",
       "1  Tim Haywood, investment director business-unit...   \n",
       "2  Nouriel Roubini, NYU professor and chairman at...   \n",
       "3  Finland's economy expanded marginally in the t...   \n",
       "4  Tourism and public spending continued to boost...   \n",
       "\n",
       "                                     Source    Topic          PublishDate  \\\n",
       "0                                 USA TODAY    obama  2002-04-02 00:00:00   \n",
       "1                                 Bloomberg  economy  2008-09-20 00:00:00   \n",
       "2                                 Bloomberg  economy  2012-01-28 00:00:00   \n",
       "3                                  RTT News  economy  2015-03-01 00:06:00   \n",
       "4  The Nation - Thailand&#39;s English news  economy  2015-03-01 00:11:00   \n",
       "\n",
       "   Facebook  GooglePlus  LinkedIn  SentimentTitle  SentimentHeadline  \n",
       "0        -1          -1        -1        0.000000          -0.053300  \n",
       "1        -1          -1        -1        0.208333          -0.156386  \n",
       "2        -1          -1        -1       -0.425210           0.139754  \n",
       "3        -1          -1        -1        0.000000           0.026064  \n",
       "4        -1          -1        -1        0.000000           0.141084  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IDLink                 0\n",
       "Title                  0\n",
       "Headline               0\n",
       "Source               175\n",
       "Topic                  0\n",
       "PublishDate            0\n",
       "Facebook               0\n",
       "GooglePlus             0\n",
       "LinkedIn               0\n",
       "SentimentTitle         0\n",
       "SentimentHeadline      0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IDLink               0\n",
       "Title                0\n",
       "Headline             0\n",
       "Source               0\n",
       "Topic                0\n",
       "PublishDate          0\n",
       "Facebook             0\n",
       "GooglePlus           0\n",
       "LinkedIn             0\n",
       "SentimentTitle       0\n",
       "SentimentHeadline    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('../ZS_associate_NLP/dataset/train_file.csv')\n",
    "test_df=pd.read_csv('../ZS_associate_NLP/dataset/test_file.csv')\n",
    "display(train_df.head(5))\n",
    "display(train_df.isna().sum())\n",
    "\n",
    "train_df['Source'] = train_df.Source.fillna(train_df[\"Source\"].mode().iloc[0])\n",
    "train_df.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display(test_df.head())\n",
    "display(test_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 792x432 with 0 Axes>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_list=train_df['Topic'].tolist()\n",
    "topic_count=Counter(topic_list)\n",
    "top_topic=topic_count.most_common()\n",
    "x,y=map(list,zip(*top_topic))\n",
    "plt.figure(figsize=(11,6))\n",
    "# sns.barplot(x=y,y=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    texts = texts.lower() \n",
    "    texts = re.sub(r'[^\\x00-\\x7F]+',' ', texts) \n",
    "    splitwords = texts.split()\n",
    "    splitwords = filter(lambda x: x[0]!= '@' , texts.split()) \n",
    "    splitwords = [word for word in splitwords if word not in set(stopwords.words('english'))] \n",
    "    texts = \" \".join(splitwords)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train_title</th>\n",
       "      <th>y_train_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obama lays wreath arlington national cemetery</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look health chinese economy</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nouriel roubini: global economy back 2008</td>\n",
       "      <td>-0.425210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finland gdp expands q4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tourism, govt spending buoys thai economy january</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       X_train_title  y_train_title\n",
       "0      obama lays wreath arlington national cemetery       0.000000\n",
       "1                        look health chinese economy       0.208333\n",
       "2          nouriel roubini: global economy back 2008      -0.425210\n",
       "3                             finland gdp expands q4       0.000000\n",
       "4  tourism, govt spending buoys thai economy january       0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_title = train_df.loc[:,'Title'].values\n",
    "y_train_title = train_df.loc[:,['SentimentTitle']].values\n",
    "\n",
    "X_train_headline = train_df.loc[:,'Headline'].values\n",
    "y_train_headline = train_df.loc[:,['SentimentHeadline']].values\n",
    "\n",
    "title_df=pd.DataFrame()\n",
    "title_df['X_train_title']=X_train_title\n",
    "title_df['y_train_title']=y_train_title\n",
    "\n",
    "headline_df=pd.DataFrame()\n",
    "headline_df['X_train_headline']=X_train_headline\n",
    "headline_df['y_train_headline']=y_train_headline\n",
    "\n",
    "title_df['X_train_title'] = title_df.X_train_title.apply(preprocess_text)\n",
    "display(title_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('V:/ML_projects/InterviewTask/NLP_pipeline/pretrain_embeddings/glove.6B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Embeddings for the titles\n",
    "\n",
    "max_len_title = title_df.X_train_title.apply(lambda x: len(x.split())).max()\n",
    "\n",
    "tok_title = Tokenizer()\n",
    "tok_title.fit_on_texts(title_df.X_train_title)\n",
    "\n",
    "vocab_size_title = len(tok_title.word_index) + 1\n",
    "\n",
    "encoded_title = tok_title.texts_to_sequences(title_df.X_train_title)\n",
    "\n",
    "padded_title = pad_sequences(encoded_title, maxlen=max_len_title, padding='post')\n",
    "\n",
    "vocab_size_title = len(tok_title.word_index) + 1\n",
    "\n",
    "title_embedding_matrix = np.zeros((vocab_size_title, 50))\n",
    "for word, i in tok_title.word_index.items():\n",
    "    t_embedding_vector = embeddings_index.get(word)\n",
    "    if t_embedding_vector is not None:\n",
    "        title_embedding_matrix[i] = t_embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Embeddings for the Headlines\n",
    "\n",
    "max_len_headline = headline_df.X_train_headline.apply(lambda x: len(x.split())).max()\n",
    "\n",
    "tok_headline = Tokenizer()\n",
    "tok_headline.fit_on_texts(headline_df.X_train_headline)\n",
    "vocab_size_headline = len(tok_headline.word_index) + 1\n",
    "encoded_headline = tok_headline.texts_to_sequences(headline_df.X_train_headline)\n",
    "padded_headline = pad_sequences(encoded_headline, maxlen=max_len_headline, padding='post')\n",
    "\n",
    "vocab_size_headline = len(tok_headline.word_index) + 1\n",
    "\n",
    "headline_embedding_matrix = np.zeros((vocab_size_headline, 50))\n",
    "for word, i in tok_headline.word_index.items():\n",
    "    h_embedding_vector = embeddings_index.get(word)\n",
    "    if h_embedding_vector is not None:\n",
    "        headline_embedding_matrix[i] = h_embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_title, x_valid_title, Y_train_title, y_valid_title = train_test_split(padded_title, y_train_title, shuffle = True, test_size = 0.15)\n",
    "x_train_headline, x_valid_headline, Y_train_headline, y_valid_headline = train_test_split(padded_headline, y_train_headline, shuffle = True, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import exp\n",
    "import keras\n",
    "# import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def mod_tanh(x):\n",
    "    return K.tanh(0.6*x)\n",
    "\n",
    "\n",
    "# config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "# sess = tf.compat.v1.Session(config=config) \n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 19, 50)            1406100   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 19, 10)            2440      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 19, 10)            0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 19, 10)           40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 19, 8)             88        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 19, 1)             9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,408,677\n",
      "Trainable params: 1,408,657\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model for title\n",
    "title_model = Sequential()\n",
    "title_model.add(Embedding(vocab_size_title, 50, input_length=max_len_title, weights=[title_embedding_matrix], trainable=True))\n",
    "title_model.add((LSTM(10, return_sequences=True)))\n",
    "title_model.add(Dropout(0.3))\n",
    "title_model.add(BatchNormalization())\n",
    "title_model.add(LSTM(20, return_sequences=True))\n",
    "title_model.add(Dropout(0.3))\n",
    "title_model.add(BatchNormalization())\n",
    "title_model.add(LSTM(20))\n",
    "title_model.add(Dropout(0.3))\n",
    "title_model.add(BatchNormalization())\n",
    "title_model.add(Dense(64, activation='relu'))\n",
    "title_model.add(Dense(8, activation='relu'))\n",
    "title_model.add(Dense(1, activation=mod_tanh))\n",
    "title_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "title_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model for Headline\n",
    "# headline_model = Sequential()\n",
    "# headline_model.add(Embedding(vocab_size_headline, 100, input_length=max_len_headline, weights=[headline_embedding_matrix], trainable=True))\n",
    "# headline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "# headline_model.add(Dropout(0.3))\n",
    "# headline_model.add(BatchNormalization())\n",
    "# headline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "# headline_model.add(Dropout(0.3))\n",
    "# headline_model.add(BatchNormalization())\n",
    "# headline_model.add(Bidirectional(LSTM(20)))\n",
    "# headline_model.add(Dropout(0.3))\n",
    "# headline_model.add(BatchNormalization())\n",
    "# headline_model.add(Dense(64, activation='relu'))\n",
    "# headline_model.add(Dense(64, activation='relu'))\n",
    "# headline_model.add(Dense(1, activation=mod_tanh))\n",
    "# headline_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_model.fit(x_train_title, Y_train_title, epochs = 1,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######CNN\n",
    "# headline_model = Sequential()\n",
    "# headline_model.add(Embedding(vocab_size_headline, 100, input_length=max_len_headline, weights=[headline_embedding_matrix], trainable=True))\n",
    "# title_model.add(Conv1D(filters=7, kernel_size=8, activation='relu'))\n",
    "# title_model.add(MaxPooling1D(pool_size=2))\n",
    "# title_model.add(Flatten())\n",
    "# title_model.add(Dense(8, activation='relu'))\n",
    "# title_model.add(Dense(1, activation=mod_tanh))\n",
    "# print(title_model.summary())\n",
    "# title_model.add(Dense(64, activation='relu'))\n",
    "# title_model.add(Dense(1, activation=mod_tanh))\n",
    "# title_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "# title_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
